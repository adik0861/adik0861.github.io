{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_WORKING_TEMPLATE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6YlEHht5tsV",
        "colab_type": "text"
      },
      "source": [
        "# Run First"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g4I76on4tyJ",
        "colab_type": "code",
        "outputId": "5a20a9f2-5253-481c-d191-37785e8f4490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/Code/CNN\n",
        "\n",
        "installed_packages = !pip list\n",
        "\n",
        "for x in ['pycuda', 'scipy', 'hiddenlayer']:\n",
        "  if x not in installed_packages:\n",
        "    print(x)\n",
        "    !pip install $x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Code/CNN\n",
            "pycuda\n",
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/35/130ac8867b30f9c6ae699b689633a803a73787533f41e52afcf28b75bd81/pycuda-2019.1.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 1.4MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2 (from pycuda)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/96/00416762a3eda8876a17d007df4a946f46b2e4ee1057e0b9714926472ef8/pytools-2019.1.1.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 24.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest>=2 in /usr/local/lib/python3.6/dist-packages (from pycuda) (3.6.4)\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.0)\n",
            "Collecting appdirs>=1.4.0 (from pycuda)\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Collecting mako (from pycuda)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz (463kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 36.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.16.4)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (7.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (41.0.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=2->pycuda) (19.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools, mako\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2019.1.1-cp36-cp36m-linux_x86_64.whl size=4544648 sha256=780802f4daa2fbda09136224ece3ef51748f749c1ff1504a51822370e20f4a77\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/f5/72/73296e7845a1ddd8769ef5baa6a8bb05bbd8baedc18184b0d1\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2019.1.1-py2.py3-none-any.whl size=58425 sha256=3ab2150881258be437753f81b3c4e32234e4bc4d726e5a1924dc08a68101ab4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/df/0b/75ac4572aaa93e3eba6a58472635d0fda907f5f4cf884a3a0c\n",
            "  Building wheel for mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mako: filename=Mako-1.1.0-cp36-none-any.whl size=75361 sha256=aa6d3e8cef1b71b0fe0546b25ae56d3d4527137e68e234d24866a9a5a8450f28\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/32/7b/a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
            "Successfully built pycuda pytools mako\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.3 mako-1.1.0 pycuda-2019.1.1 pytools-2019.1.1\n",
            "scipy\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.4)\n",
            "hiddenlayer\n",
            "Collecting hiddenlayer\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/80/f284e0441945341c2fe669d70a17277746baf891fe4e517dcbe7784cbf24/hiddenlayer-0.2-py3-none-any.whl\n",
            "Installing collected packages: hiddenlayer\n",
            "Successfully installed hiddenlayer-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGz6jmY16ovD",
        "colab_type": "text"
      },
      "source": [
        "# Import Packages, Load Dataset, Define Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bYB7sHe6Tji",
        "colab_type": "code",
        "outputId": "7949768e-5170-40ad-d952-3fc8f6145e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ModelName = 'ConvNet' # [ConvNet] [GoogleNet]\n",
        "img_dir = './data' # Specificy path to CIFAR-10 dataset and set download yes/no flag\n",
        "import os, sys\n",
        "# Check current directory isn't root\n",
        "if os.getcwd() == '/content':\n",
        "  %cd /content/drive/My\\ Drive/Code/CNN\n",
        "\n",
        "import hiddenlayer as hl  \n",
        "import pickle\n",
        "from torchsummary import summary\n",
        "from timeit import default_timer as timer\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lrs\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "cuda.init()\n",
        "## Get Id of default device\n",
        "torch.cuda.current_device()\n",
        "cuda.Device(0).name()\n",
        "print(cuda.Device(0).name())\n",
        "\n",
        "############## CONFIGURATION ##############\n",
        "NUM_WORKERS = 8\n",
        "#### Adaptive LR specific parameters\n",
        "LR_PATIENCE = 5 # This is the number of epochs to observe no change in before change LR\n",
        "LR_FACTOR = 0.75  # factor by which to reduce the LR newLR = LR*LF_FACTOR\n",
        "BATCH_SIZE = 128\n",
        "train_iterations = int((50000/BATCH_SIZE)) \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# DEFINING TRANSFORM TO APPLY TO THE IMAGES\n",
        "train_transform = transforms.Compose([\n",
        "   transforms.RandomCrop(32, padding=4),\n",
        "   transforms.RandomHorizontalFlip(),\n",
        "   transforms.Resize(32),\n",
        "\t transforms.ToTensor(),\n",
        "\t transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "   transforms.Resize(32),\n",
        "\t transforms.ToTensor(),\n",
        "\t transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "########################################################################\n",
        "# 1. LOAD AND NORMALIZE CIFAR10 DATASET\n",
        "########################################################################\n",
        "# Specify training and testing sets\n",
        "trainset = torchvision.datasets.CIFAR10(root = img_dir, train = True, download = False, transform = train_transform)\n",
        "testset = torchvision.datasets.CIFAR10(root = img_dir, train = False, download = False, transform = test_transform)\n",
        "# Shuffle the training set only\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = BATCH_SIZE, shuffle = False, num_workers = NUM_WORKERS)\n",
        "\n",
        "def unpickle(file):\n",
        "\twith open(file, 'rb') as fo:\n",
        "\t\tmeta_dict_b = pickle.load(fo, encoding = 'bytes')\n",
        "\t\tmeta_dict = [x.decode('UTF-8') for x in meta_dict_b[b'label_names']]\n",
        "\treturn meta_dict\n",
        "\n",
        "def get_lr(optimizer):\n",
        "\tfor param_group in optimizer.param_groups:\n",
        "\t\treturn param_group['lr']    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMIoB9K-6FIq",
        "colab_type": "text"
      },
      "source": [
        "# Define Both Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7CZQLG06D5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################################################\n",
        "# 2. DEFINE YOUR CONVOLUTIONAL NEURAL NETWORK\n",
        "########################################################################  \n",
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n",
        "        super(Inception, self).__init__()\n",
        "        # 1x1 conv branch\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, kernel_1_x, kernel_size=1),\n",
        "            nn.BatchNorm2d(kernel_1_x),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 1x1 conv -> 3x3 conv branch\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, kernel_3_in, kernel_size=1),\n",
        "            nn.BatchNorm2d(kernel_3_in),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(kernel_3_in, kernel_3_x, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(kernel_3_x),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 1x1 conv -> 5x5 conv branch\n",
        "        self.b3 = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, kernel_5_in, kernel_size=1),\n",
        "            nn.BatchNorm2d(kernel_5_in),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(kernel_5_in, kernel_5_x, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(kernel_5_x),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(kernel_5_x, kernel_5_x, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(kernel_5_x),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        # 3x3 pool -> 1x1 conv branch\n",
        "        self.b4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
        "            nn.BatchNorm2d(pool_planes),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.b1(x)\n",
        "        y2 = self.b2(x)\n",
        "        y3 = self.b3(x)\n",
        "        y4 = self.b4(x)\n",
        "        return torch.cat([y1,y2,y3,y4], 1)\n",
        "\n",
        "\n",
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        self.pre_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
        "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
        "\n",
        "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
        "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
        "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
        "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
        "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
        "\n",
        "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.linear = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pre_layers(x)\n",
        "        x = self.a3(x)\n",
        "        x = self.b3(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.a4(x)\n",
        "        x = self.b4(x)\n",
        "        x = self.c4(x)\n",
        "        x = self.d4(x)\n",
        "        x = self.e4(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.a5(x)\n",
        "        x = self.b5(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "\tdef __init__(self, init_weights = False):\n",
        "\t\tsuper(ConvNet, self).__init__()\n",
        "\t\t\n",
        "\t\tself.conv1 = nn.Conv2d(3, WEIGHTS, 3)\n",
        "\t\tself.bnorm1 = nn.BatchNorm2d(WEIGHTS)\n",
        "\t\tself.conv2 = nn.Conv2d(WEIGHTS, WEIGHTS, 3)\n",
        "\t\tself.bnorm2 = nn.BatchNorm2d(WEIGHTS)\n",
        "\t\tself.drop_out1 = nn.Dropout(p = 0.2)\n",
        "\t\t\n",
        "\t\tself.conv3 = nn.Conv2d(WEIGHTS, WEIGHTS * 2, 3)\n",
        "\t\tself.bnorm3 = nn.BatchNorm2d(WEIGHTS * 2)\n",
        "\t\tself.conv4 = nn.Conv2d(WEIGHTS * 2, WEIGHTS * 2, 3)\n",
        "\t\tself.bnorm4 = nn.BatchNorm2d(WEIGHTS * 2)\n",
        "\t\tself.drop_out2 = nn.Dropout(p = 0.3)\n",
        "\t\t\n",
        "\t\tself.conv5 = nn.Conv2d(WEIGHTS * 2, WEIGHTS * 4, 3)\n",
        "\t\tself.bnorm5 = nn.BatchNorm2d(WEIGHTS * 4)\n",
        "\t\tself.conv6 = nn.Conv2d(WEIGHTS * 4, WEIGHTS * 4, 3)\n",
        "\t\tself.bnorm6 = nn.BatchNorm2d(WEIGHTS * 4)\n",
        "\t\tself.drop_out3 = nn.Dropout(p = 0.4)\n",
        "\t\t\n",
        "\t\tself.fc1 = nn.Linear((WEIGHTS * 4) * 4 * 4, 10)\n",
        "\t\tself.pool = nn.MaxPool2d(2, 2)  # 2x2 window with stride = 2\n",
        "\t\tself.softmax = nn.Softmax()\n",
        "\t\t\n",
        "\t\tif init_weights:\n",
        "\t\t\tself._initialize_weights()\n",
        "\t\n",
        "\tdef forward(self, x):\n",
        "\t\t# Pad following pooling in order to maintaing 32x32 size\n",
        "\t\tpad1 = [1, 1, 1, 1]  # use below padding for alternate kernel sizes in subsequent layers--e.g. 3-4\n",
        "\t\t\n",
        "\t\tx = F.pad(self.bnorm1((F.relu(self.conv1(x)))), pad1, 'constant', 0)\n",
        "\t\tx = F.pad(self.bnorm2(F.relu(self.conv2(x))), pad1, 'constant', 0)\n",
        "\t\tx = self.drop_out1(self.pool(x))\n",
        "\t\t\n",
        "\t\tx = F.pad(self.bnorm3((F.relu(self.conv3(x)))), pad1, 'constant', 0)\n",
        "\t\tx = F.pad(self.bnorm4(F.relu(self.conv4(x))), pad1, 'constant', 0)\n",
        "\t\tx = self.drop_out2(self.pool(x))\n",
        "\t\t\n",
        "\t\tx = F.pad(self.bnorm5((F.relu(self.conv5(x)))), pad1, 'constant', 0)\n",
        "\t\tx = F.pad(self.bnorm6(F.relu(self.conv6(x))), pad1, 'constant', 0)\n",
        "\t\tx = self.drop_out3(self.pool(x))\n",
        "\t\t\n",
        "\t\tx = x.view(-1, (WEIGHTS * 4) * 4 * 4)\n",
        "\t\tx = self.fc1(x)\n",
        "#\t\tx = self.softmax(x)\n",
        "\t\treturn x\n",
        "\t\n",
        "\tdef _initialize_weights(self):\n",
        "\t\tfor m in self.modules():\n",
        "\t\t\tif isinstance(m, nn.Conv2d):\n",
        "\t\t\t\tnn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\t\t\telif isinstance(m, nn.BatchNorm2d):\n",
        "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\t\t\telif isinstance(m, nn.Linear):\n",
        "\t\t\t\tnn.init.normal_(m.weight, 0, 0.01)\n",
        "\t\t\t\tnn.init.constant_(m.bias, 0)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwwjgDbZ8fdd",
        "colab_type": "text"
      },
      "source": [
        "# main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUyeiKTr8hrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hiddenlayer\n",
        "\n",
        "def main():\n",
        "\t########################################################################\n",
        "\t# 3. DEFINE A LOSS FUNCTION AND OPTIMIZER\n",
        "\t########################################################################\n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\toptimizer = optim.SGD(net.parameters(), lr = LR, momentum = 0.9, weight_decay = 0.0001)\n",
        "\t # Adaptive learning rate--if accuracy (over 2 epochs) doesn't increase then reduce it by 10%\n",
        "\tscheduler = lrs.ReduceLROnPlateau(optimizer, 'max',\n",
        "\t                                   factor = LR_FACTOR,\n",
        "\t                                   patience = LR_PATIENCE,\n",
        "\t                                   verbose = True,\n",
        "\t                                   threshold = 1)\n",
        "\t# scheduler = CyclicLR(optimizer, base_lr = 0.0001, max_lr = 0.1, step_size = half_cycle)  \n",
        "\t########################################################################\n",
        "\t# 4. TRAIN THE NETWORK\n",
        "\t########################################################################\n",
        "\t# Get class labels\n",
        "\tmeta = img_dir + '/cifar-10-batches-py/batches.meta'\n",
        "\tclasses = unpickle(meta)\n",
        "\t# Print model hyper-parameters and hidden parameters\n",
        "\tprint('=' * 100)\n",
        "\tprint('Classes: [%s]' % ', '.join(map(str, classes)))\n",
        "\tprint('Number of Filters: ' + str(WEIGHTS))\n",
        "\tprint('Batchsize = ' + str(BATCH_SIZE))\n",
        "\tprint('Number of Batches = ' + str(train_iterations))\n",
        "\tsummary(net, (3, 32, 32))\n",
        "\tprint('=' * 100)\n",
        "\ttest_accuracy = []\n",
        "\ttrain_accuracy = []\n",
        "\ttrain_loss = []\n",
        "\thistory2 = hl.History()\n",
        "\tcanvas2 = hl.Canvas() \n",
        "\tstep = (0,0)\n",
        "    \n",
        "\tfor epoch in range(1, EPOCH_NUM, 1):  # loop over the dataset multiple times\n",
        "\t\t# scheduler.batch_step()  # uncomment for CyclicLR\n",
        "\t\tprint('Beginning Epoch ' + str(epoch) + ' with Learning Rate = ' + str(round(get_lr(optimizer), 5)))\n",
        "\t\trunning_loss = 0.0\n",
        "\t\ttest_min_acc = 0\n",
        "\t\ttotal = 0\n",
        "\t\tcorrect = 0\n",
        "\t\tfor i, Data in enumerate(trainloader, 0):\n",
        "\t\t\tstep = (epoch, i)\n",
        "      # get the inputs\n",
        "\t\t\tinputs, labels = Data\n",
        "\t\t\t# zero the parameter gradients\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\t# forward + backward + optimize\n",
        "\t\t\toutputs = net(inputs.to(device)).to(device)\n",
        "\t\t\tloss = criterion(outputs, labels.to(device))\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t# print statistics\n",
        "\t\t\trunning_loss += loss.item()\n",
        "\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t# Accuracy of given batch\n",
        "\t\t\ttotal += labels.size(0)\n",
        "\t\t\tcorrect += (predicted == labels.to(device)).sum().item()\n",
        "\t\t\ttrain_loss.append(running_loss / 20)\n",
        "\t\t\ttrain_accuracy.append(100.0 * correct / total)\n",
        "\t\t\taccuracy = 100.0 * correct / total\n",
        "\t\t\tif i % 20 == 19:  # print every 20 mini-batches\n",
        "\t\t\t\tprint('Train: [%d, %5d] Loss: %.3f Acc: %.3f' % (epoch, i + 1, running_loss / 20, accuracy))\n",
        "\t\t\t\trunning_loss = 0.0\n",
        "\t\t\t\n",
        "\t\t# TEST LEARNT MODEL ON TEST-SET\n",
        "\t\tcorrect = 0\n",
        "\t\ttotal = 0\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor Data in testloader:\n",
        "\t\t\t\timages, labels = Data\n",
        "\t\t\t\toutputs = net(images.to(device))\n",
        "\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
        "\t\t\t\ttotal += labels.to(device).size(0)\n",
        "\t\t\t\tcorrect += (predicted == labels.to(device)).sum().item()\n",
        "\t\ttest_accuracy.append(100.0 * correct / total)\n",
        "\t\ttest_ep_acc = test_accuracy[-1]\n",
        "\t\t# See if LR needs to be changed--used by ReduceLROnPlateau above\n",
        "\t\tscheduler.step(test_ep_acc)\n",
        "\t\t\n",
        "\t\ttest_acc_str = '[ Epoch ' + str(epoch) + ' Test Accuracy = ' + str(round(test_ep_acc, 3)) + ' % ]'\n",
        "\t\tpad = 50 - int(round(float(len(test_acc_str) / 2)))\n",
        "\t\tprint('=' * pad + test_acc_str + '=' * pad)\n",
        "\t\t# SAVE BEST MODEL\n",
        "\t\tif test_min_acc < test_ep_acc:\n",
        "\t\t\ttest_min_acc = test_ep_acc\n",
        "\t\t\ttorch.save(net, MODEL_SAVE_PATH + '/my_best_model.pth')\n",
        "\t\n",
        "\tnp.save('test_accuracy.npy', test_accuracy);\n",
        "\tsio.savemat('test_accuracy.mat', mdict = {'test_accuracy': test_accuracy})\n",
        "\tnp.save('train_accuracy.npy', train_accuracy);\n",
        "\tsio.savemat('train_accuracy.mat', mdict = {'train_accuracy': train_accuracy})\n",
        "\tnp.save('train_loss.npy', train_loss);\n",
        "\tsio.savemat('train_loss.mat', mdict = {'train_loss': train_loss})\n",
        "\t\n",
        "\tprint('Finished Training')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znw0miuq5x4o",
        "colab_type": "text"
      },
      "source": [
        "# Define Hyper-Parameters & Execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqyetvfA52te",
        "colab_type": "code",
        "outputId": "faa80157-24be-443c-bfe2-7cf029ee0c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ModelName = 'GoogleNet'\n",
        "\n",
        "######################################################\n",
        "################## HYPER-PARAMETERS ##################\n",
        "######################################################\n",
        "WEIGHTS = 32\n",
        "EPOCH_NUM = 15\n",
        "LR = 0.1 # NOTE: LR is adaptive\n",
        "\n",
        "######################################################\n",
        "################ CREATE MODEL SUB-DIR ################\n",
        "######################################################\n",
        "img_dir = './data' # Specificy path to CIFAR-10 dataset and set download yes/no flag\n",
        "MODEL_SAVE_PATH = './Output/'+ ModelName\n",
        "cwd_path = os.getcwd()\n",
        "if cwd_path == '/content': # Check current directory isn't root\n",
        "    %cd /content/drive/My\\ Drive/Code/CNN\n",
        "mkdir_var = str(cwd_path +  '/Output/' + ModelName + '/Models').replace(\" \", \"\\ \")\n",
        "!mkdir -p $mkdir_var\n",
        "# mkdir_var = str(cwd_path +  '/data/ImageNet/').replace(\" \", \"\\ \")\n",
        "# !mkdir -p $mkdir_var\n",
        "print(MODEL_SAVE_PATH)\n",
        "\n",
        "\n",
        "\n",
        "######################################################\n",
        "##################### EXECUTE ########################\n",
        "######################################################\n",
        "if __name__ == \"__main__\":\n",
        "    if ModelName == 'GoogleNet': net = GoogLeNet().to(device)\n",
        "    else: net = ConvNet().to(device)\n",
        "    start = timer()\n",
        "    main()\n",
        "    end = timer()\n",
        "    execution_time = timedelta(seconds=end-start)\n",
        "    print(execution_time)\n",
        "    file = open(MODEL_SAVE_PATH + '/ExecutionTime.txt','w')\n",
        "    file.writelines(str(execution_time)) \n",
        "    file.close() \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./Output/GoogleNet\n",
            "====================================================================================================\n",
            "Classes: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n",
            "Number of Filters: 32\n",
            "Batchsize = 128\n",
            "Number of Batches = 390\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 192, 32, 32]           5,376\n",
            "       BatchNorm2d-2          [-1, 192, 32, 32]             384\n",
            "              ReLU-3          [-1, 192, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          12,352\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 96, 32, 32]          18,528\n",
            "       BatchNorm2d-8           [-1, 96, 32, 32]             192\n",
            "              ReLU-9           [-1, 96, 32, 32]               0\n",
            "           Conv2d-10          [-1, 128, 32, 32]         110,720\n",
            "      BatchNorm2d-11          [-1, 128, 32, 32]             256\n",
            "             ReLU-12          [-1, 128, 32, 32]               0\n",
            "           Conv2d-13           [-1, 16, 32, 32]           3,088\n",
            "      BatchNorm2d-14           [-1, 16, 32, 32]              32\n",
            "             ReLU-15           [-1, 16, 32, 32]               0\n",
            "           Conv2d-16           [-1, 32, 32, 32]           4,640\n",
            "      BatchNorm2d-17           [-1, 32, 32, 32]              64\n",
            "             ReLU-18           [-1, 32, 32, 32]               0\n",
            "           Conv2d-19           [-1, 32, 32, 32]           9,248\n",
            "      BatchNorm2d-20           [-1, 32, 32, 32]              64\n",
            "             ReLU-21           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-22          [-1, 192, 32, 32]               0\n",
            "           Conv2d-23           [-1, 32, 32, 32]           6,176\n",
            "      BatchNorm2d-24           [-1, 32, 32, 32]              64\n",
            "             ReLU-25           [-1, 32, 32, 32]               0\n",
            "        Inception-26          [-1, 256, 32, 32]               0\n",
            "           Conv2d-27          [-1, 128, 32, 32]          32,896\n",
            "      BatchNorm2d-28          [-1, 128, 32, 32]             256\n",
            "             ReLU-29          [-1, 128, 32, 32]               0\n",
            "           Conv2d-30          [-1, 128, 32, 32]          32,896\n",
            "      BatchNorm2d-31          [-1, 128, 32, 32]             256\n",
            "             ReLU-32          [-1, 128, 32, 32]               0\n",
            "           Conv2d-33          [-1, 192, 32, 32]         221,376\n",
            "      BatchNorm2d-34          [-1, 192, 32, 32]             384\n",
            "             ReLU-35          [-1, 192, 32, 32]               0\n",
            "           Conv2d-36           [-1, 32, 32, 32]           8,224\n",
            "      BatchNorm2d-37           [-1, 32, 32, 32]              64\n",
            "             ReLU-38           [-1, 32, 32, 32]               0\n",
            "           Conv2d-39           [-1, 96, 32, 32]          27,744\n",
            "      BatchNorm2d-40           [-1, 96, 32, 32]             192\n",
            "             ReLU-41           [-1, 96, 32, 32]               0\n",
            "           Conv2d-42           [-1, 96, 32, 32]          83,040\n",
            "      BatchNorm2d-43           [-1, 96, 32, 32]             192\n",
            "             ReLU-44           [-1, 96, 32, 32]               0\n",
            "        MaxPool2d-45          [-1, 256, 32, 32]               0\n",
            "           Conv2d-46           [-1, 64, 32, 32]          16,448\n",
            "      BatchNorm2d-47           [-1, 64, 32, 32]             128\n",
            "             ReLU-48           [-1, 64, 32, 32]               0\n",
            "        Inception-49          [-1, 480, 32, 32]               0\n",
            "        MaxPool2d-50          [-1, 480, 16, 16]               0\n",
            "           Conv2d-51          [-1, 192, 16, 16]          92,352\n",
            "      BatchNorm2d-52          [-1, 192, 16, 16]             384\n",
            "             ReLU-53          [-1, 192, 16, 16]               0\n",
            "           Conv2d-54           [-1, 96, 16, 16]          46,176\n",
            "      BatchNorm2d-55           [-1, 96, 16, 16]             192\n",
            "             ReLU-56           [-1, 96, 16, 16]               0\n",
            "           Conv2d-57          [-1, 208, 16, 16]         179,920\n",
            "      BatchNorm2d-58          [-1, 208, 16, 16]             416\n",
            "             ReLU-59          [-1, 208, 16, 16]               0\n",
            "           Conv2d-60           [-1, 16, 16, 16]           7,696\n",
            "      BatchNorm2d-61           [-1, 16, 16, 16]              32\n",
            "             ReLU-62           [-1, 16, 16, 16]               0\n",
            "           Conv2d-63           [-1, 48, 16, 16]           6,960\n",
            "      BatchNorm2d-64           [-1, 48, 16, 16]              96\n",
            "             ReLU-65           [-1, 48, 16, 16]               0\n",
            "           Conv2d-66           [-1, 48, 16, 16]          20,784\n",
            "      BatchNorm2d-67           [-1, 48, 16, 16]              96\n",
            "             ReLU-68           [-1, 48, 16, 16]               0\n",
            "        MaxPool2d-69          [-1, 480, 16, 16]               0\n",
            "           Conv2d-70           [-1, 64, 16, 16]          30,784\n",
            "      BatchNorm2d-71           [-1, 64, 16, 16]             128\n",
            "             ReLU-72           [-1, 64, 16, 16]               0\n",
            "        Inception-73          [-1, 512, 16, 16]               0\n",
            "           Conv2d-74          [-1, 160, 16, 16]          82,080\n",
            "      BatchNorm2d-75          [-1, 160, 16, 16]             320\n",
            "             ReLU-76          [-1, 160, 16, 16]               0\n",
            "           Conv2d-77          [-1, 112, 16, 16]          57,456\n",
            "      BatchNorm2d-78          [-1, 112, 16, 16]             224\n",
            "             ReLU-79          [-1, 112, 16, 16]               0\n",
            "           Conv2d-80          [-1, 224, 16, 16]         226,016\n",
            "      BatchNorm2d-81          [-1, 224, 16, 16]             448\n",
            "             ReLU-82          [-1, 224, 16, 16]               0\n",
            "           Conv2d-83           [-1, 24, 16, 16]          12,312\n",
            "      BatchNorm2d-84           [-1, 24, 16, 16]              48\n",
            "             ReLU-85           [-1, 24, 16, 16]               0\n",
            "           Conv2d-86           [-1, 64, 16, 16]          13,888\n",
            "      BatchNorm2d-87           [-1, 64, 16, 16]             128\n",
            "             ReLU-88           [-1, 64, 16, 16]               0\n",
            "           Conv2d-89           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-90           [-1, 64, 16, 16]             128\n",
            "             ReLU-91           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-92          [-1, 512, 16, 16]               0\n",
            "           Conv2d-93           [-1, 64, 16, 16]          32,832\n",
            "      BatchNorm2d-94           [-1, 64, 16, 16]             128\n",
            "             ReLU-95           [-1, 64, 16, 16]               0\n",
            "        Inception-96          [-1, 512, 16, 16]               0\n",
            "           Conv2d-97          [-1, 128, 16, 16]          65,664\n",
            "      BatchNorm2d-98          [-1, 128, 16, 16]             256\n",
            "             ReLU-99          [-1, 128, 16, 16]               0\n",
            "          Conv2d-100          [-1, 128, 16, 16]          65,664\n",
            "     BatchNorm2d-101          [-1, 128, 16, 16]             256\n",
            "            ReLU-102          [-1, 128, 16, 16]               0\n",
            "          Conv2d-103          [-1, 256, 16, 16]         295,168\n",
            "     BatchNorm2d-104          [-1, 256, 16, 16]             512\n",
            "            ReLU-105          [-1, 256, 16, 16]               0\n",
            "          Conv2d-106           [-1, 24, 16, 16]          12,312\n",
            "     BatchNorm2d-107           [-1, 24, 16, 16]              48\n",
            "            ReLU-108           [-1, 24, 16, 16]               0\n",
            "          Conv2d-109           [-1, 64, 16, 16]          13,888\n",
            "     BatchNorm2d-110           [-1, 64, 16, 16]             128\n",
            "            ReLU-111           [-1, 64, 16, 16]               0\n",
            "          Conv2d-112           [-1, 64, 16, 16]          36,928\n",
            "     BatchNorm2d-113           [-1, 64, 16, 16]             128\n",
            "            ReLU-114           [-1, 64, 16, 16]               0\n",
            "       MaxPool2d-115          [-1, 512, 16, 16]               0\n",
            "          Conv2d-116           [-1, 64, 16, 16]          32,832\n",
            "     BatchNorm2d-117           [-1, 64, 16, 16]             128\n",
            "            ReLU-118           [-1, 64, 16, 16]               0\n",
            "       Inception-119          [-1, 512, 16, 16]               0\n",
            "          Conv2d-120          [-1, 112, 16, 16]          57,456\n",
            "     BatchNorm2d-121          [-1, 112, 16, 16]             224\n",
            "            ReLU-122          [-1, 112, 16, 16]               0\n",
            "          Conv2d-123          [-1, 144, 16, 16]          73,872\n",
            "     BatchNorm2d-124          [-1, 144, 16, 16]             288\n",
            "            ReLU-125          [-1, 144, 16, 16]               0\n",
            "          Conv2d-126          [-1, 288, 16, 16]         373,536\n",
            "     BatchNorm2d-127          [-1, 288, 16, 16]             576\n",
            "            ReLU-128          [-1, 288, 16, 16]               0\n",
            "          Conv2d-129           [-1, 32, 16, 16]          16,416\n",
            "     BatchNorm2d-130           [-1, 32, 16, 16]              64\n",
            "            ReLU-131           [-1, 32, 16, 16]               0\n",
            "          Conv2d-132           [-1, 64, 16, 16]          18,496\n",
            "     BatchNorm2d-133           [-1, 64, 16, 16]             128\n",
            "            ReLU-134           [-1, 64, 16, 16]               0\n",
            "          Conv2d-135           [-1, 64, 16, 16]          36,928\n",
            "     BatchNorm2d-136           [-1, 64, 16, 16]             128\n",
            "            ReLU-137           [-1, 64, 16, 16]               0\n",
            "       MaxPool2d-138          [-1, 512, 16, 16]               0\n",
            "          Conv2d-139           [-1, 64, 16, 16]          32,832\n",
            "     BatchNorm2d-140           [-1, 64, 16, 16]             128\n",
            "            ReLU-141           [-1, 64, 16, 16]               0\n",
            "       Inception-142          [-1, 528, 16, 16]               0\n",
            "          Conv2d-143          [-1, 256, 16, 16]         135,424\n",
            "     BatchNorm2d-144          [-1, 256, 16, 16]             512\n",
            "            ReLU-145          [-1, 256, 16, 16]               0\n",
            "          Conv2d-146          [-1, 160, 16, 16]          84,640\n",
            "     BatchNorm2d-147          [-1, 160, 16, 16]             320\n",
            "            ReLU-148          [-1, 160, 16, 16]               0\n",
            "          Conv2d-149          [-1, 320, 16, 16]         461,120\n",
            "     BatchNorm2d-150          [-1, 320, 16, 16]             640\n",
            "            ReLU-151          [-1, 320, 16, 16]               0\n",
            "          Conv2d-152           [-1, 32, 16, 16]          16,928\n",
            "     BatchNorm2d-153           [-1, 32, 16, 16]              64\n",
            "            ReLU-154           [-1, 32, 16, 16]               0\n",
            "          Conv2d-155          [-1, 128, 16, 16]          36,992\n",
            "     BatchNorm2d-156          [-1, 128, 16, 16]             256\n",
            "            ReLU-157          [-1, 128, 16, 16]               0\n",
            "          Conv2d-158          [-1, 128, 16, 16]         147,584\n",
            "     BatchNorm2d-159          [-1, 128, 16, 16]             256\n",
            "            ReLU-160          [-1, 128, 16, 16]               0\n",
            "       MaxPool2d-161          [-1, 528, 16, 16]               0\n",
            "          Conv2d-162          [-1, 128, 16, 16]          67,712\n",
            "     BatchNorm2d-163          [-1, 128, 16, 16]             256\n",
            "            ReLU-164          [-1, 128, 16, 16]               0\n",
            "       Inception-165          [-1, 832, 16, 16]               0\n",
            "       MaxPool2d-166            [-1, 832, 8, 8]               0\n",
            "          Conv2d-167            [-1, 256, 8, 8]         213,248\n",
            "     BatchNorm2d-168            [-1, 256, 8, 8]             512\n",
            "            ReLU-169            [-1, 256, 8, 8]               0\n",
            "          Conv2d-170            [-1, 160, 8, 8]         133,280\n",
            "     BatchNorm2d-171            [-1, 160, 8, 8]             320\n",
            "            ReLU-172            [-1, 160, 8, 8]               0\n",
            "          Conv2d-173            [-1, 320, 8, 8]         461,120\n",
            "     BatchNorm2d-174            [-1, 320, 8, 8]             640\n",
            "            ReLU-175            [-1, 320, 8, 8]               0\n",
            "          Conv2d-176             [-1, 32, 8, 8]          26,656\n",
            "     BatchNorm2d-177             [-1, 32, 8, 8]              64\n",
            "            ReLU-178             [-1, 32, 8, 8]               0\n",
            "          Conv2d-179            [-1, 128, 8, 8]          36,992\n",
            "     BatchNorm2d-180            [-1, 128, 8, 8]             256\n",
            "            ReLU-181            [-1, 128, 8, 8]               0\n",
            "          Conv2d-182            [-1, 128, 8, 8]         147,584\n",
            "     BatchNorm2d-183            [-1, 128, 8, 8]             256\n",
            "            ReLU-184            [-1, 128, 8, 8]               0\n",
            "       MaxPool2d-185            [-1, 832, 8, 8]               0\n",
            "          Conv2d-186            [-1, 128, 8, 8]         106,624\n",
            "     BatchNorm2d-187            [-1, 128, 8, 8]             256\n",
            "            ReLU-188            [-1, 128, 8, 8]               0\n",
            "       Inception-189            [-1, 832, 8, 8]               0\n",
            "          Conv2d-190            [-1, 384, 8, 8]         319,872\n",
            "     BatchNorm2d-191            [-1, 384, 8, 8]             768\n",
            "            ReLU-192            [-1, 384, 8, 8]               0\n",
            "          Conv2d-193            [-1, 192, 8, 8]         159,936\n",
            "     BatchNorm2d-194            [-1, 192, 8, 8]             384\n",
            "            ReLU-195            [-1, 192, 8, 8]               0\n",
            "          Conv2d-196            [-1, 384, 8, 8]         663,936\n",
            "     BatchNorm2d-197            [-1, 384, 8, 8]             768\n",
            "            ReLU-198            [-1, 384, 8, 8]               0\n",
            "          Conv2d-199             [-1, 48, 8, 8]          39,984\n",
            "     BatchNorm2d-200             [-1, 48, 8, 8]              96\n",
            "            ReLU-201             [-1, 48, 8, 8]               0\n",
            "          Conv2d-202            [-1, 128, 8, 8]          55,424\n",
            "     BatchNorm2d-203            [-1, 128, 8, 8]             256\n",
            "            ReLU-204            [-1, 128, 8, 8]               0\n",
            "          Conv2d-205            [-1, 128, 8, 8]         147,584\n",
            "     BatchNorm2d-206            [-1, 128, 8, 8]             256\n",
            "            ReLU-207            [-1, 128, 8, 8]               0\n",
            "       MaxPool2d-208            [-1, 832, 8, 8]               0\n",
            "          Conv2d-209            [-1, 128, 8, 8]         106,624\n",
            "     BatchNorm2d-210            [-1, 128, 8, 8]             256\n",
            "            ReLU-211            [-1, 128, 8, 8]               0\n",
            "       Inception-212           [-1, 1024, 8, 8]               0\n",
            "       AvgPool2d-213           [-1, 1024, 1, 1]               0\n",
            "          Linear-214                   [-1, 10]          10,250\n",
            "----------------------------------------------------------------\n",
            "Total params: 6,166,250\n",
            "Trainable params: 6,166,250\n",
            "Non-trainable params: 0\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 81.42\n",
            "Params size (MB): 23.52\n",
            "Estimated Total Size (MB): 104.96\n",
            "====================================================================================================\n",
            "Beginning Epoch 1 with Learning Rate = 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f0c396851eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mexecution_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-05589d329ac0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}