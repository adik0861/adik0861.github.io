{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MobileNet_v3.ipynb","provenance":[{"file_id":"1Jy2HPgm6EKoGjfT62x7DVGogDtU46vZH","timestamp":1571266781723}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BsAg1T10lJK4","colab_type":"text"},"source":["# **Prelim**"]},{"cell_type":"markdown","metadata":{"id":"wuAaBadGk5WT","colab_type":"text"},"source":["## GoogleDrive"]},{"cell_type":"code","metadata":{"id":"1UbSBCUdoMex","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"35ff79b2-d789-4882-f863-265887329eb1","executionInfo":{"status":"ok","timestamp":1572333581024,"user_tz":420,"elapsed":241877,"user":{"displayName":"Aditya Kunapuli","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDdP8koEInjmAwkN_phGm2BjZqMmpOhU2nNPAiIWg=s64","userId":"12383070090400352620"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd /content/drive/My\\ Drive/Code/Project/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/Code/Project\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p0vsAiRW_heG","colab_type":"text"},"source":["## Install Packages"]},{"cell_type":"code","metadata":{"id":"-jXduRQKoMhP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"7b67490e-427f-4219-e490-18f413fc6c70","executionInfo":{"status":"ok","timestamp":1572333735519,"user_tz":420,"elapsed":396338,"user":{"displayName":"Aditya Kunapuli","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDdP8koEInjmAwkN_phGm2BjZqMmpOhU2nNPAiIWg=s64","userId":"12383070090400352620"}}},"source":["# %%capture\n","installed_packages = !pip list\n","# required_packages = ['pycuda', 'scipy', 'hiddenlayer', 'sklearn']\n","required_packages = ['pycuda']\n","for x in required_packages:\n","  if x not in installed_packages:\n","    print(x)\n","    null_capture = !pip install $x\n","\n","print('Packages checked/installed')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["pycuda\n","scipy\n","hiddenlayer\n","Packages checked/installed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hvB9byoV_nLG","colab_type":"text"},"source":["# Model, Dataset and Hyperparameters"]},{"cell_type":"code","metadata":{"id":"DHO8c_7e_sd3","colab_type":"code","colab":{}},"source":["model_name = 'MobileNetV2'\n","data_name = 'CIFAR10'\n","model_name = model_name.lower()\n","\n","WEIGHTS = 32\n","EPOCH_NUM = 30\n","\n","NUM_WORKERS = 8\n","BATCH_SIZE = 16\n","\n","HP_momentum = 0.9\n","HP_weightdecay = 0.00004\n","\n","LR = 0.1 # NOTE: LR is adaptive\n","# Adaptive LR specific parameters\n","LR_PATIENCE = 3 # This is the number of epochs to observe no change in before change LR\n","LR_FACTOR = 0.9  # factor by which to reduce the learning rate: newLR = oldLR*LF_FACTOR"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZ-sQ9VR_YG9","colab_type":"text"},"source":["# Load Data and Model"]},{"cell_type":"code","metadata":{"id":"k5Mj31cSvIhz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"e3fd7087-0c93-419f-f893-f3bf61a4c693","executionInfo":{"status":"ok","timestamp":1572333787815,"user_tz":420,"elapsed":3319,"user":{"displayName":"Aditya Kunapuli","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDdP8koEInjmAwkN_phGm2BjZqMmpOhU2nNPAiIWg=s64","userId":"12383070090400352620"}}},"source":["import model_Load\n","import importlib\n","importlib.reload(model_Load) # forces a refresh in case of any changes to imported py package\n","\n","LoadModel = model_Load.LoadModel()\n","# Get model-specific transforms\n","trainT, testT = LoadModel.transform_type(model_name)\n","# Load chosen dataset\n","train_loader, test_loader, classes = LoadModel.load_data(dataset_name=data_name,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\ttransform_train=trainT, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\ttransform_test=testT, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatchsize=BATCH_SIZE, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tnumworkers=NUM_WORKERS)\n","# Load chosen model\n","if ('mobile' in model_name and 'v2' not in model_name):\n","\tfrom model_MobileNet import MobileNet as model\n","\tprint('MobileNet')\n","elif ('mobile' in model_name and 'v2' in model_name):\n","\tfrom torchvision.models import mobilenet_v2 as model\n","\tprint('MobileNetV2')\n","elif 'conv' in model_name:\n","\tfrom model_ConvNet import ConvNet as model\n","\tprint('ConvNet')\n","elif 'google' in model_name:\n","\tfrom model_GoogleNet import GoogleNet as model\n","\tprint('GoogleNet')\n","else: \n","\tprint('Choose a correct model type: MobileNet, MobileNetV2, GoogleNet, ConvNet')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["mobile\n","Dataset: CIFAR10\n","Classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n","MobileNetV2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_sucWROy5ksm","colab_type":"text"},"source":["# Define the *```main```* function"]},{"cell_type":"code","metadata":{"id":"dK0ZzxmL5pfe","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lrs\n","from torchsummary import summary\n","from timeit import default_timer as timer\n","from datetime import timedelta\n","import math\n","import os\n","import numpy as np\n","from scipy import io as sio\n","import pycuda.driver as cuda\n","cuda.init()\n","torch.cuda.current_device() \n","cuda.Device(0).name()\n","\n","def main():\n","\t########################################################################\n","\t# Define Loss function and optimizer\n","\t########################################################################\n","\tcriterion = nn.CrossEntropyLoss()\n","\toptimizer = optim.SGD(net.parameters(), lr = LR, momentum = HP_momentum, weight_decay = HP_weightdecay)\n","\t # Adaptive learning rate--if accuracy (over 2 epochs) doesn't increase then reduce it by 10%\n","\tscheduler = lrs.ReduceLROnPlateau(optimizer, 'max',\n","\t\t\t\t\t\t\t\t\t   factor = LR_FACTOR,\n","\t\t\t\t\t\t\t\t\t   patience = LR_PATIENCE,\n","\t\t\t\t\t\t\t\t\t   verbose = True,\n","\t\t\t\t\t\t\t\t\t   threshold = 1)\n","\t# scheduler = CyclicLR(optimizer, base_lr = 0.0001, max_lr = 0.1, step_size = half_cycle)  \n","\t########################################################################\n","\t# Begin training\n","\t########################################################################\n","\t# Print model hyper-parameters and hidden parameters\n","\tprint('=' * 100)\n","\tprint('Classes: [%s]' % ', '.join(map(str, classes)))\n","\tprint('Number of Filters: ' + str(WEIGHTS))\n","\tprint('Batchsize = ' + str(BATCH_SIZE))\n","\tprint('Number of Batches = ' + str(train_iterations))\n","\tsummary(net, (3, 224, 224))\n","\tprint('=' * 100)\n","\ttest_accuracy = []\n","\ttrain_accuracy = []\n","\ttrain_loss = []\n","\t# history2 = hl.History()\n","\t# canvas2 = hl.Canvas() \n","\tstep = (0,0)\n","\t# net.load_state_dict(torch.load(checkpoint_path))\n","\tfor epoch in range(1, EPOCH_NUM, 1):  \n","\t\t# scheduler.batch_step()  # uncomment for CyclicLR\n","\t\tprint('Beginning Epoch ' + str(epoch) + ' with Learning Rate = ' + str(round(get_lr(optimizer), 5)))\n","\t\trunning_loss, test_min_acc, total, correct = (0.0, 0.0, 0, 0)\n","\t\tfor i, Data in enumerate(train_loader, 0):\n","\t\t\tstep = (epoch, i)\n","\t\t\tnet.train()\n","\t\t\t# get the inputs\n","\t\t\tinputs, labels = Data\n","\t\t\t# zero the parameter gradients\n","\t\t\toptimizer.zero_grad()\n","\t\t\t# forward + backward + optimize\n","\t\t\toutputs = net(inputs.to(device)).to(device)\n","\t\t\tloss = criterion(outputs, labels.to(device))\n","\t\t\tloss.backward()\n","\t\t\toptimizer.step()\n","\t\t\t# print statistics\n","\t\t\trunning_loss += loss.item()\n","\t\t\tnet.eval()\n","\t\t\t_, predicted = torch.max(outputs.data, 1)\n","\t\t\t# Accuracy of given batch\n","\t\t\ttotal += labels.size(0)\n","\t\t\tcorrect += (predicted == labels.to(device)).sum().item()\n","\t\t\ttrain_loss.append(running_loss / 20)\n","\t\t\ttrain_accuracy.append(100.0 * correct / total)\n","\t\t\taccuracy = 100.0 * correct / total\n","\n","\t\t\tif i % 200 == 0:  # print every 20 mini-batches\n","\t\t\t\tprint('Train: [%d, %5d] Loss: %.3f Acc: %.3f' % (epoch, i + 1, running_loss / 20, accuracy))\n","\t\t\t\trunning_loss = 0.0\n","\t\t\t\n","\t\t# TEST LEARNT MODEL ON TEST-SET\n","\t\tcorrect, total = (0, 0)\n","\t\twith torch.no_grad():\n","\t\t\tnet.eval()\n","\t\t\tfor Data in test_loader:\n","\t\t\t\timages, labels = Data\n","\t\t\t\toutputs = net(images.to(device))\n","\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n","\t\t\t\ttotal += labels.to(device).size(0)\n","\t\t\t\tcorrect += (predicted == labels.to(device)).sum().item()\n","\t\ttest_accuracy.append(100.0 * correct / total)\n","\t\ttest_ep_acc = test_accuracy[-1]\n","\t\t# See if LR needs to be changed--used by ReduceLROnPlateau above\n","\t\tscheduler.step(test_ep_acc)\n","\t\t\n","\t\ttest_acc_str = '[ Epoch ' + str(epoch) + ' Test Accuracy = ' + str(round(test_ep_acc, 3)) + ' % ]'\n","\t\tpad = 50 - int(round(float(len(test_acc_str) / 2)))\n","\t\tprint('=' * pad + test_acc_str + '=' * pad)\n","\t\t# SAVE BEST MODEL\n","\t\tif test_min_acc < test_ep_acc:\n","\t\t\ttest_min_acc = test_ep_acc\n","\t\t\ttorch.save(net, MODEL_SAVE_PATH + '/my_best_model.pth')\n","\t\n","\tnp.save('test_accuracy.npy', test_accuracy);\n","\tsio.savemat('test_accuracy.mat', mdict = {'test_accuracy': test_accuracy})\n","\tnp.save('train_accuracy.npy', train_accuracy);\n","\tsio.savemat('train_accuracy.mat', mdict = {'train_accuracy': train_accuracy})\n","\tnp.save('train_loss.npy', train_loss);\n","\tsio.savemat('train_loss.mat', mdict = {'train_loss': train_loss})\n","\t\n","\tprint('Finished Training')  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_Uy6wco6L5n","colab_type":"text"},"source":["# Execute"]},{"cell_type":"code","metadata":{"id":"U30Px85r6MIl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"295ad352-ff36-40f6-d701-d69bc4a603cb","executionInfo":{"status":"ok","timestamp":1572350747950,"user_tz":420,"elapsed":6849604,"user":{"displayName":"Aditya Kunapuli","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDdP8koEInjmAwkN_phGm2BjZqMmpOhU2nNPAiIWg=s64","userId":"12383070090400352620"}}},"source":["train_iterations = int((50000/BATCH_SIZE)) \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","def get_lr(optimizer):\n","\tfor param_group in optimizer.param_groups:\n","\t\treturn param_group['lr']    \n","\n","######################################################\n","################ CREATE MODEL SUB-DIR ################\n","######################################################\n","img_dir = './data' # Specificy path to CIFAR-10 dataset and set download yes/no flag\n","MODEL_SAVE_PATH = './Output/'+ model_name\n","cwd_path = os.getcwd()\n","if cwd_path == '/content': # Check current directory isn't root\n","    %cd /content/drive/My\\ Drive/Code/Project\n","mkdir_var = str(cwd_path +  '/Output/' + model_name + '/Models').replace(\" \", \"\\ \")\n","!mkdir -p $mkdir_var\n","print(MODEL_SAVE_PATH)\n","# checkpoint_path = MODEL_SAVE_PATH + '/' + model_name + '.pth'\n","checkpoint_path = MODEL_SAVE_PATH + '/my_best_model.pth'\n","\n","\n","\n","######################################################\n","##################### EXECUTE ########################\n","######################################################\n","if __name__ == \"__main__\":\n","    # net = model().to(device)\n","    # net.load_state_dict(torch.load(checkpoint_path))\n","    net = torch.load(checkpoint_path)\n","    start = timer()\n","    main()\n","    end = timer()\n","    execution_time = timedelta(seconds=end-start)\n","    print(execution_time)\n","    # torch.save(net.state_dict(), checkpoint_path)\n","    file = open(MODEL_SAVE_PATH + '/ExecutionTime.txt','w')\n","    file.writelines(str(execution_time)) \n","    file.close() \n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["./Output/mobilenetv2\n","====================================================================================================\n","Classes: [plane, car, bird, cat, deer, dog, frog, horse, ship, truck]\n","Number of Filters: 32\n","Batchsize = 16\n","Number of Batches = 3125\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 112, 112]             864\n","       BatchNorm2d-2         [-1, 32, 112, 112]              64\n","             ReLU6-3         [-1, 32, 112, 112]               0\n","            Conv2d-4         [-1, 32, 112, 112]             288\n","       BatchNorm2d-5         [-1, 32, 112, 112]              64\n","             ReLU6-6         [-1, 32, 112, 112]               0\n","            Conv2d-7         [-1, 16, 112, 112]             512\n","       BatchNorm2d-8         [-1, 16, 112, 112]              32\n","  InvertedResidual-9         [-1, 16, 112, 112]               0\n","           Conv2d-10         [-1, 96, 112, 112]           1,536\n","      BatchNorm2d-11         [-1, 96, 112, 112]             192\n","            ReLU6-12         [-1, 96, 112, 112]               0\n","           Conv2d-13           [-1, 96, 56, 56]             864\n","      BatchNorm2d-14           [-1, 96, 56, 56]             192\n","            ReLU6-15           [-1, 96, 56, 56]               0\n","           Conv2d-16           [-1, 24, 56, 56]           2,304\n","      BatchNorm2d-17           [-1, 24, 56, 56]              48\n"," InvertedResidual-18           [-1, 24, 56, 56]               0\n","           Conv2d-19          [-1, 144, 56, 56]           3,456\n","      BatchNorm2d-20          [-1, 144, 56, 56]             288\n","            ReLU6-21          [-1, 144, 56, 56]               0\n","           Conv2d-22          [-1, 144, 56, 56]           1,296\n","      BatchNorm2d-23          [-1, 144, 56, 56]             288\n","            ReLU6-24          [-1, 144, 56, 56]               0\n","           Conv2d-25           [-1, 24, 56, 56]           3,456\n","      BatchNorm2d-26           [-1, 24, 56, 56]              48\n"," InvertedResidual-27           [-1, 24, 56, 56]               0\n","           Conv2d-28          [-1, 144, 56, 56]           3,456\n","      BatchNorm2d-29          [-1, 144, 56, 56]             288\n","            ReLU6-30          [-1, 144, 56, 56]               0\n","           Conv2d-31          [-1, 144, 28, 28]           1,296\n","      BatchNorm2d-32          [-1, 144, 28, 28]             288\n","            ReLU6-33          [-1, 144, 28, 28]               0\n","           Conv2d-34           [-1, 32, 28, 28]           4,608\n","      BatchNorm2d-35           [-1, 32, 28, 28]              64\n"," InvertedResidual-36           [-1, 32, 28, 28]               0\n","           Conv2d-37          [-1, 192, 28, 28]           6,144\n","      BatchNorm2d-38          [-1, 192, 28, 28]             384\n","            ReLU6-39          [-1, 192, 28, 28]               0\n","           Conv2d-40          [-1, 192, 28, 28]           1,728\n","      BatchNorm2d-41          [-1, 192, 28, 28]             384\n","            ReLU6-42          [-1, 192, 28, 28]               0\n","           Conv2d-43           [-1, 32, 28, 28]           6,144\n","      BatchNorm2d-44           [-1, 32, 28, 28]              64\n"," InvertedResidual-45           [-1, 32, 28, 28]               0\n","           Conv2d-46          [-1, 192, 28, 28]           6,144\n","      BatchNorm2d-47          [-1, 192, 28, 28]             384\n","            ReLU6-48          [-1, 192, 28, 28]               0\n","           Conv2d-49          [-1, 192, 28, 28]           1,728\n","      BatchNorm2d-50          [-1, 192, 28, 28]             384\n","            ReLU6-51          [-1, 192, 28, 28]               0\n","           Conv2d-52           [-1, 32, 28, 28]           6,144\n","      BatchNorm2d-53           [-1, 32, 28, 28]              64\n"," InvertedResidual-54           [-1, 32, 28, 28]               0\n","           Conv2d-55          [-1, 192, 28, 28]           6,144\n","      BatchNorm2d-56          [-1, 192, 28, 28]             384\n","            ReLU6-57          [-1, 192, 28, 28]               0\n","           Conv2d-58          [-1, 192, 14, 14]           1,728\n","      BatchNorm2d-59          [-1, 192, 14, 14]             384\n","            ReLU6-60          [-1, 192, 14, 14]               0\n","           Conv2d-61           [-1, 64, 14, 14]          12,288\n","      BatchNorm2d-62           [-1, 64, 14, 14]             128\n"," InvertedResidual-63           [-1, 64, 14, 14]               0\n","           Conv2d-64          [-1, 384, 14, 14]          24,576\n","      BatchNorm2d-65          [-1, 384, 14, 14]             768\n","            ReLU6-66          [-1, 384, 14, 14]               0\n","           Conv2d-67          [-1, 384, 14, 14]           3,456\n","      BatchNorm2d-68          [-1, 384, 14, 14]             768\n","            ReLU6-69          [-1, 384, 14, 14]               0\n","           Conv2d-70           [-1, 64, 14, 14]          24,576\n","      BatchNorm2d-71           [-1, 64, 14, 14]             128\n"," InvertedResidual-72           [-1, 64, 14, 14]               0\n","           Conv2d-73          [-1, 384, 14, 14]          24,576\n","      BatchNorm2d-74          [-1, 384, 14, 14]             768\n","            ReLU6-75          [-1, 384, 14, 14]               0\n","           Conv2d-76          [-1, 384, 14, 14]           3,456\n","      BatchNorm2d-77          [-1, 384, 14, 14]             768\n","            ReLU6-78          [-1, 384, 14, 14]               0\n","           Conv2d-79           [-1, 64, 14, 14]          24,576\n","      BatchNorm2d-80           [-1, 64, 14, 14]             128\n"," InvertedResidual-81           [-1, 64, 14, 14]               0\n","           Conv2d-82          [-1, 384, 14, 14]          24,576\n","      BatchNorm2d-83          [-1, 384, 14, 14]             768\n","            ReLU6-84          [-1, 384, 14, 14]               0\n","           Conv2d-85          [-1, 384, 14, 14]           3,456\n","      BatchNorm2d-86          [-1, 384, 14, 14]             768\n","            ReLU6-87          [-1, 384, 14, 14]               0\n","           Conv2d-88           [-1, 64, 14, 14]          24,576\n","      BatchNorm2d-89           [-1, 64, 14, 14]             128\n"," InvertedResidual-90           [-1, 64, 14, 14]               0\n","           Conv2d-91          [-1, 384, 14, 14]          24,576\n","      BatchNorm2d-92          [-1, 384, 14, 14]             768\n","            ReLU6-93          [-1, 384, 14, 14]               0\n","           Conv2d-94          [-1, 384, 14, 14]           3,456\n","      BatchNorm2d-95          [-1, 384, 14, 14]             768\n","            ReLU6-96          [-1, 384, 14, 14]               0\n","           Conv2d-97           [-1, 96, 14, 14]          36,864\n","      BatchNorm2d-98           [-1, 96, 14, 14]             192\n"," InvertedResidual-99           [-1, 96, 14, 14]               0\n","          Conv2d-100          [-1, 576, 14, 14]          55,296\n","     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n","           ReLU6-102          [-1, 576, 14, 14]               0\n","          Conv2d-103          [-1, 576, 14, 14]           5,184\n","     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n","           ReLU6-105          [-1, 576, 14, 14]               0\n","          Conv2d-106           [-1, 96, 14, 14]          55,296\n","     BatchNorm2d-107           [-1, 96, 14, 14]             192\n","InvertedResidual-108           [-1, 96, 14, 14]               0\n","          Conv2d-109          [-1, 576, 14, 14]          55,296\n","     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n","           ReLU6-111          [-1, 576, 14, 14]               0\n","          Conv2d-112          [-1, 576, 14, 14]           5,184\n","     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n","           ReLU6-114          [-1, 576, 14, 14]               0\n","          Conv2d-115           [-1, 96, 14, 14]          55,296\n","     BatchNorm2d-116           [-1, 96, 14, 14]             192\n","InvertedResidual-117           [-1, 96, 14, 14]               0\n","          Conv2d-118          [-1, 576, 14, 14]          55,296\n","     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n","           ReLU6-120          [-1, 576, 14, 14]               0\n","          Conv2d-121            [-1, 576, 7, 7]           5,184\n","     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n","           ReLU6-123            [-1, 576, 7, 7]               0\n","          Conv2d-124            [-1, 160, 7, 7]          92,160\n","     BatchNorm2d-125            [-1, 160, 7, 7]             320\n","InvertedResidual-126            [-1, 160, 7, 7]               0\n","          Conv2d-127            [-1, 960, 7, 7]         153,600\n","     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n","           ReLU6-129            [-1, 960, 7, 7]               0\n","          Conv2d-130            [-1, 960, 7, 7]           8,640\n","     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n","           ReLU6-132            [-1, 960, 7, 7]               0\n","          Conv2d-133            [-1, 160, 7, 7]         153,600\n","     BatchNorm2d-134            [-1, 160, 7, 7]             320\n","InvertedResidual-135            [-1, 160, 7, 7]               0\n","          Conv2d-136            [-1, 960, 7, 7]         153,600\n","     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n","           ReLU6-138            [-1, 960, 7, 7]               0\n","          Conv2d-139            [-1, 960, 7, 7]           8,640\n","     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n","           ReLU6-141            [-1, 960, 7, 7]               0\n","          Conv2d-142            [-1, 160, 7, 7]         153,600\n","     BatchNorm2d-143            [-1, 160, 7, 7]             320\n","InvertedResidual-144            [-1, 160, 7, 7]               0\n","          Conv2d-145            [-1, 960, 7, 7]         153,600\n","     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n","           ReLU6-147            [-1, 960, 7, 7]               0\n","          Conv2d-148            [-1, 960, 7, 7]           8,640\n","     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n","           ReLU6-150            [-1, 960, 7, 7]               0\n","          Conv2d-151            [-1, 320, 7, 7]         307,200\n","     BatchNorm2d-152            [-1, 320, 7, 7]             640\n","InvertedResidual-153            [-1, 320, 7, 7]               0\n","          Conv2d-154           [-1, 1280, 7, 7]         409,600\n","     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n","           ReLU6-156           [-1, 1280, 7, 7]               0\n","         Dropout-157                 [-1, 1280]               0\n","          Linear-158                 [-1, 1000]       1,281,000\n","================================================================\n","Total params: 3,504,872\n","Trainable params: 3,504,872\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 152.87\n","Params size (MB): 13.37\n","Estimated Total Size (MB): 166.81\n","----------------------------------------------------------------\n","====================================================================================================\n","Beginning Epoch 1 with Learning Rate = 0.1\n","Train: [1,     1] Loss: 0.012 Acc: 93.750\n","Train: [1,   201] Loss: 5.077 Acc: 82.836\n","Train: [1,   401] Loss: 5.530 Acc: 82.076\n","Train: [1,   601] Loss: 5.535 Acc: 81.552\n","Train: [1,   801] Loss: 5.802 Acc: 81.211\n","Train: [1,  1001] Loss: 5.737 Acc: 81.056\n","Train: [1,  1201] Loss: 5.280 Acc: 81.198\n","Train: [1,  1401] Loss: 5.671 Acc: 81.170\n","Train: [1,  1601] Loss: 5.891 Acc: 81.039\n","Train: [1,  1801] Loss: 5.764 Acc: 81.042\n","Train: [1,  2001] Loss: 5.857 Acc: 80.981\n","Train: [1,  2201] Loss: 5.612 Acc: 80.972\n","Train: [1,  2401] Loss: 5.846 Acc: 80.888\n","Train: [1,  2601] Loss: 5.720 Acc: 80.854\n","Train: [1,  2801] Loss: 5.681 Acc: 80.862\n","Train: [1,  3001] Loss: 5.754 Acc: 80.813\n","================================[ Epoch 1 Test Accuracy = 81.27 % ]================================\n","Beginning Epoch 2 with Learning Rate = 0.1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MobileNetV2. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvBNReLU. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU6. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InvertedResidual. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train: [2,     1] Loss: 0.021 Acc: 87.500\n","Train: [2,   201] Loss: 5.403 Acc: 81.437\n","Train: [2,   401] Loss: 5.256 Acc: 81.842\n","Train: [2,   601] Loss: 5.207 Acc: 81.968\n","Train: [2,   801] Loss: 5.462 Acc: 81.710\n","Train: [2,  1001] Loss: 6.049 Acc: 81.188\n","Train: [2,  1201] Loss: 5.230 Acc: 81.260\n","Train: [2,  1401] Loss: 5.567 Acc: 81.223\n","Train: [2,  1601] Loss: 5.269 Acc: 81.387\n","Train: [2,  1801] Loss: 5.629 Acc: 81.340\n","Train: [2,  2001] Loss: 5.581 Acc: 81.234\n","Train: [2,  2201] Loss: 5.568 Acc: 81.182\n","Train: [2,  2401] Loss: 5.399 Acc: 81.190\n","Train: [2,  2601] Loss: 5.519 Acc: 81.214\n","Train: [2,  2801] Loss: 5.476 Acc: 81.230\n","Train: [2,  3001] Loss: 5.578 Acc: 81.244\n","================================[ Epoch 2 Test Accuracy = 81.28 % ]================================\n","Beginning Epoch 3 with Learning Rate = 0.1\n","Train: [3,     1] Loss: 0.039 Acc: 68.750\n","Train: [3,   201] Loss: 4.602 Acc: 83.644\n","Train: [3,   401] Loss: 5.038 Acc: 83.370\n","Train: [3,   601] Loss: 4.894 Acc: 83.174\n","Train: [3,   801] Loss: 5.648 Acc: 82.709\n","Train: [3,  1001] Loss: 5.410 Acc: 82.486\n","Train: [3,  1201] Loss: 5.515 Acc: 82.249\n","Train: [3,  1401] Loss: 5.374 Acc: 82.133\n","Train: [3,  1601] Loss: 5.457 Acc: 82.027\n","Train: [3,  1801] Loss: 5.195 Acc: 82.097\n","Train: [3,  2001] Loss: 5.531 Acc: 81.993\n","Train: [3,  2201] Loss: 5.503 Acc: 81.940\n","Train: [3,  2401] Loss: 5.591 Acc: 81.867\n","Train: [3,  2601] Loss: 5.395 Acc: 81.901\n","Train: [3,  2801] Loss: 5.253 Acc: 81.924\n","Train: [3,  3001] Loss: 5.316 Acc: 81.971\n","================================[ Epoch 3 Test Accuracy = 81.46 % ]================================\n","Beginning Epoch 4 with Learning Rate = 0.1\n","Train: [4,     1] Loss: 0.024 Acc: 93.750\n","Train: [4,   201] Loss: 5.248 Acc: 81.654\n","Train: [4,   401] Loss: 4.840 Acc: 82.606\n","Train: [4,   601] Loss: 5.074 Acc: 82.633\n","Train: [4,   801] Loss: 5.459 Acc: 82.436\n","Train: [4,  1001] Loss: 5.127 Acc: 82.486\n","Train: [4,  1201] Loss: 5.538 Acc: 82.291\n","Train: [4,  1401] Loss: 5.051 Acc: 82.446\n","Train: [4,  1601] Loss: 5.162 Acc: 82.511\n","Train: [4,  1801] Loss: 5.239 Acc: 82.510\n","Train: [4,  2001] Loss: 4.975 Acc: 82.562\n","Train: [4,  2201] Loss: 5.665 Acc: 82.383\n","Train: [4,  2401] Loss: 5.442 Acc: 82.276\n","Train: [4,  2601] Loss: 5.402 Acc: 82.259\n","Train: [4,  2801] Loss: 5.268 Acc: 82.196\n","Train: [4,  3001] Loss: 5.482 Acc: 82.146\n","Epoch     3: reducing learning rate of group 0 to 9.0000e-02.\n","================================[ Epoch 4 Test Accuracy = 80.37 % ]================================\n","Beginning Epoch 5 with Learning Rate = 0.09\n","Train: [5,     1] Loss: 0.020 Acc: 87.500\n","Train: [5,   201] Loss: 4.549 Acc: 85.044\n","Train: [5,   401] Loss: 4.345 Acc: 85.131\n","Train: [5,   601] Loss: 4.885 Acc: 84.536\n","Train: [5,   801] Loss: 4.908 Acc: 84.223\n","Train: [5,  1001] Loss: 5.113 Acc: 84.035\n","Train: [5,  1201] Loss: 4.728 Acc: 83.993\n","Train: [5,  1401] Loss: 4.729 Acc: 84.007\n","Train: [5,  1601] Loss: 5.066 Acc: 83.819\n","Train: [5,  1801] Loss: 5.017 Acc: 83.752\n","Train: [5,  2001] Loss: 4.803 Acc: 83.652\n","Train: [5,  2201] Loss: 4.926 Acc: 83.613\n","Train: [5,  2401] Loss: 5.123 Acc: 83.569\n","Train: [5,  2601] Loss: 4.707 Acc: 83.595\n","Train: [5,  2801] Loss: 5.131 Acc: 83.551\n","Train: [5,  3001] Loss: 5.118 Acc: 83.456\n","================================[ Epoch 5 Test Accuracy = 79.33 % ]================================\n","Beginning Epoch 6 with Learning Rate = 0.09\n","Train: [6,     1] Loss: 0.015 Acc: 93.750\n","Train: [6,   201] Loss: 4.395 Acc: 86.039\n","Train: [6,   401] Loss: 4.337 Acc: 85.427\n","Train: [6,   601] Loss: 4.602 Acc: 84.983\n","Train: [6,   801] Loss: 4.997 Acc: 84.449\n","Train: [6,  1001] Loss: 4.753 Acc: 84.291\n","Train: [6,  1201] Loss: 5.032 Acc: 84.159\n","Train: [6,  1401] Loss: 4.746 Acc: 83.994\n","Train: [6,  1601] Loss: 4.792 Acc: 83.940\n","Train: [6,  1801] Loss: 5.107 Acc: 83.776\n","Train: [6,  2001] Loss: 4.891 Acc: 83.752\n","Train: [6,  2201] Loss: 4.838 Acc: 83.684\n","Train: [6,  2401] Loss: 4.910 Acc: 83.694\n","Train: [6,  2601] Loss: 5.114 Acc: 83.576\n","Train: [6,  2801] Loss: 4.510 Acc: 83.633\n","Train: [6,  3001] Loss: 4.804 Acc: 83.616\n","=================================[ Epoch 6 Test Accuracy = 82.3 % ]=================================\n","Beginning Epoch 7 with Learning Rate = 0.09\n","Train: [7,     1] Loss: 0.015 Acc: 93.750\n","Train: [7,   201] Loss: 4.108 Acc: 86.039\n","Train: [7,   401] Loss: 4.799 Acc: 84.679\n","Train: [7,   601] Loss: 4.789 Acc: 84.235\n","Train: [7,   801] Loss: 4.382 Acc: 84.675\n","Train: [7,  1001] Loss: 4.635 Acc: 84.590\n","Train: [7,  1201] Loss: 4.938 Acc: 84.440\n","Train: [7,  1401] Loss: 4.809 Acc: 84.417\n","Train: [7,  1601] Loss: 4.899 Acc: 84.303\n","Train: [7,  1801] Loss: 4.969 Acc: 84.169\n","Train: [7,  2001] Loss: 4.609 Acc: 84.205\n","Train: [7,  2201] Loss: 5.055 Acc: 84.084\n","Train: [7,  2401] Loss: 5.064 Acc: 83.981\n","Train: [7,  2601] Loss: 5.249 Acc: 83.857\n","Train: [7,  2801] Loss: 4.843 Acc: 83.852\n","Train: [7,  3001] Loss: 4.955 Acc: 83.789\n","Epoch     6: reducing learning rate of group 0 to 8.1000e-02.\n","================================[ Epoch 7 Test Accuracy = 83.16 % ]================================\n","Beginning Epoch 8 with Learning Rate = 0.081\n","Train: [8,     1] Loss: 0.011 Acc: 87.500\n","Train: [8,   201] Loss: 4.346 Acc: 84.826\n","Train: [8,   401] Loss: 4.305 Acc: 84.975\n","Train: [8,   601] Loss: 4.379 Acc: 84.723\n","Train: [8,   801] Loss: 4.307 Acc: 84.886\n","Train: [8,  1001] Loss: 4.328 Acc: 84.903\n","Train: [8,  1201] Loss: 4.544 Acc: 84.804\n","Train: [8,  1401] Loss: 4.185 Acc: 84.926\n","Train: [8,  1601] Loss: 4.348 Acc: 84.994\n","Train: [8,  1801] Loss: 4.614 Acc: 84.949\n","Train: [8,  2001] Loss: 4.430 Acc: 84.976\n","Train: [8,  2201] Loss: 4.534 Acc: 84.959\n","Train: [8,  2401] Loss: 4.767 Acc: 84.840\n","Train: [8,  2601] Loss: 4.382 Acc: 84.850\n","Train: [8,  2801] Loss: 4.620 Acc: 84.836\n","Train: [8,  3001] Loss: 4.691 Acc: 84.730\n","================================[ Epoch 8 Test Accuracy = 82.47 % ]================================\n","Beginning Epoch 9 with Learning Rate = 0.081\n","Train: [9,     1] Loss: 0.027 Acc: 68.750\n","Train: [9,   201] Loss: 4.113 Acc: 85.665\n","Train: [9,   401] Loss: 3.891 Acc: 86.191\n","Train: [9,   601] Loss: 4.117 Acc: 86.179\n","Train: [9,   801] Loss: 4.308 Acc: 85.838\n","Train: [9,  1001] Loss: 4.476 Acc: 85.746\n","Train: [9,  1201] Loss: 4.589 Acc: 85.455\n","Train: [9,  1401] Loss: 4.510 Acc: 85.381\n","Train: [9,  1601] Loss: 4.393 Acc: 85.435\n","Train: [9,  1801] Loss: 4.578 Acc: 85.352\n","Train: [9,  2001] Loss: 4.665 Acc: 85.179\n","Train: [9,  2201] Loss: 4.674 Acc: 85.058\n","Train: [9,  2401] Loss: 4.432 Acc: 85.097\n","Train: [9,  2601] Loss: 4.350 Acc: 85.080\n","Train: [9,  2801] Loss: 4.977 Acc: 84.909\n","Train: [9,  3001] Loss: 4.402 Acc: 84.901\n","================================[ Epoch 9 Test Accuracy = 82.14 % ]================================\n","Beginning Epoch 10 with Learning Rate = 0.081\n","Train: [10,     1] Loss: 0.027 Acc: 81.250\n","Train: [10,   201] Loss: 4.050 Acc: 86.101\n","Train: [10,   401] Loss: 3.985 Acc: 86.222\n","Train: [10,   601] Loss: 4.217 Acc: 85.919\n","Train: [10,   801] Loss: 4.543 Acc: 85.705\n","Train: [10,  1001] Loss: 4.534 Acc: 85.433\n","Train: [10,  1201] Loss: 4.305 Acc: 85.304\n","Train: [10,  1401] Loss: 4.609 Acc: 85.176\n","Train: [10,  1601] Loss: 4.558 Acc: 85.072\n","Train: [10,  1801] Loss: 4.526 Acc: 85.050\n","Train: [10,  2001] Loss: 4.572 Acc: 84.976\n","Train: [10,  2201] Loss: 4.328 Acc: 85.001\n","Train: [10,  2401] Loss: 4.524 Acc: 84.965\n","Train: [10,  2601] Loss: 4.859 Acc: 84.797\n","Train: [10,  2801] Loss: 4.743 Acc: 84.735\n","Train: [10,  3001] Loss: 5.010 Acc: 84.659\n","Epoch     9: reducing learning rate of group 0 to 7.2900e-02.\n","================================[ Epoch 10 Test Accuracy = 83.8 % ]================================\n","Beginning Epoch 11 with Learning Rate = 0.0729\n","Train: [11,     1] Loss: 0.005 Acc: 100.000\n","Train: [11,   201] Loss: 3.766 Acc: 86.878\n","Train: [11,   401] Loss: 3.775 Acc: 87.313\n","Train: [11,   601] Loss: 4.031 Acc: 86.803\n","Train: [11,   801] Loss: 3.996 Acc: 86.650\n","Train: [11,  1001] Loss: 4.112 Acc: 86.451\n","Train: [11,  1201] Loss: 4.157 Acc: 86.329\n","Train: [11,  1401] Loss: 3.761 Acc: 86.461\n","Train: [11,  1601] Loss: 4.187 Acc: 86.458\n","Train: [11,  1801] Loss: 4.454 Acc: 86.289\n","Train: [11,  2001] Loss: 4.401 Acc: 86.113\n","Train: [11,  2201] Loss: 4.483 Acc: 85.950\n","Train: [11,  2401] Loss: 4.319 Acc: 85.876\n","Train: [11,  2601] Loss: 4.534 Acc: 85.784\n","Train: [11,  2801] Loss: 4.192 Acc: 85.737\n","Train: [11,  3001] Loss: 4.309 Acc: 85.669\n","================================[ Epoch 11 Test Accuracy = 84.12 % ]================================\n","Beginning Epoch 12 with Learning Rate = 0.0729\n","Train: [12,     1] Loss: 0.026 Acc: 81.250\n","Train: [12,   201] Loss: 3.602 Acc: 87.034\n","Train: [12,   401] Loss: 3.700 Acc: 87.297\n","Train: [12,   601] Loss: 4.037 Acc: 86.834\n","Train: [12,   801] Loss: 4.115 Acc: 86.642\n","Train: [12,  1001] Loss: 4.221 Acc: 86.414\n","Train: [12,  1201] Loss: 4.204 Acc: 86.272\n","Train: [12,  1401] Loss: 3.882 Acc: 86.322\n","Train: [12,  1601] Loss: 4.044 Acc: 86.302\n","Train: [12,  1801] Loss: 4.278 Acc: 86.157\n","Train: [12,  2001] Loss: 3.988 Acc: 86.148\n","Train: [12,  2201] Loss: 4.212 Acc: 86.046\n","Train: [12,  2401] Loss: 4.019 Acc: 86.060\n","Train: [12,  2601] Loss: 4.301 Acc: 86.017\n","Train: [12,  2801] Loss: 4.074 Acc: 86.076\n","Train: [12,  3001] Loss: 4.526 Acc: 85.980\n","================================[ Epoch 12 Test Accuracy = 84.99 % ]================================\n","Beginning Epoch 13 with Learning Rate = 0.0729\n","Train: [13,     1] Loss: 0.017 Acc: 87.500\n","Train: [13,   201] Loss: 3.639 Acc: 87.034\n","Train: [13,   401] Loss: 3.707 Acc: 87.126\n","Train: [13,   601] Loss: 3.841 Acc: 86.928\n","Train: [13,   801] Loss: 3.828 Acc: 86.806\n","Train: [13,  1001] Loss: 4.057 Acc: 86.726\n","Train: [13,  1201] Loss: 4.142 Acc: 86.537\n","Train: [13,  1401] Loss: 4.122 Acc: 86.438\n","Train: [13,  1601] Loss: 3.994 Acc: 86.458\n","Train: [13,  1801] Loss: 4.104 Acc: 86.393\n","Train: [13,  2001] Loss: 4.295 Acc: 86.229\n","Train: [13,  2201] Loss: 4.223 Acc: 86.185\n","Train: [13,  2401] Loss: 4.624 Acc: 86.024\n","Train: [13,  2601] Loss: 4.154 Acc: 86.010\n","Train: [13,  2801] Loss: 4.228 Acc: 86.047\n","Train: [13,  3001] Loss: 4.227 Acc: 85.990\n","Epoch    12: reducing learning rate of group 0 to 6.5610e-02.\n","================================[ Epoch 13 Test Accuracy = 83.5 % ]================================\n","Beginning Epoch 14 with Learning Rate = 0.06561\n","Train: [14,     1] Loss: 0.016 Acc: 81.250\n","Train: [14,   201] Loss: 3.573 Acc: 87.655\n","Train: [14,   401] Loss: 3.468 Acc: 87.749\n","Train: [14,   601] Loss: 3.763 Acc: 87.438\n","Train: [14,   801] Loss: 3.599 Acc: 87.617\n","Train: [14,  1001] Loss: 3.721 Acc: 87.619\n","Train: [14,  1201] Loss: 3.842 Acc: 87.432\n","Train: [14,  1401] Loss: 3.662 Acc: 87.362\n","Train: [14,  1601] Loss: 3.656 Acc: 87.309\n","Train: [14,  1801] Loss: 3.808 Acc: 87.271\n","Train: [14,  2001] Loss: 3.952 Acc: 87.266\n","Train: [14,  2201] Loss: 4.360 Acc: 87.066\n","Train: [14,  2401] Loss: 4.133 Acc: 86.946\n","Train: [14,  2601] Loss: 3.844 Acc: 86.945\n","Train: [14,  2801] Loss: 4.144 Acc: 86.857\n","Train: [14,  3001] Loss: 4.026 Acc: 86.846\n","================================[ Epoch 14 Test Accuracy = 84.61 % ]================================\n","Finished Training\n","1:54:06.777943\n"],"name":"stdout"}]}]}