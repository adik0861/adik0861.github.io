{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MobileNet_v3.ipynb","provenance":[{"file_id":"1Jy2HPgm6EKoGjfT62x7DVGogDtU46vZH","timestamp":1571266781723}],"collapsed_sections":["p0vsAiRW_heG","8sALciwOXggi","_sucWROy5ksm"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BsAg1T10lJK4","colab_type":"text"},"source":["# **Prelim**"]},{"cell_type":"markdown","metadata":{"id":"wuAaBadGk5WT","colab_type":"text"},"source":["## GoogleDrive"]},{"cell_type":"code","metadata":{"id":"1UbSBCUdoMex","colab_type":"code","outputId":"d60b6dd9-a440-44b5-aa83-698522aa8119","executionInfo":{"status":"ok","timestamp":1572486403422,"user_tz":420,"elapsed":2421,"user":{"displayName":"Aditya Kunapuli","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDdP8koEInjmAwkN_phGm2BjZqMmpOhU2nNPAiIWg=s64","userId":"12383070090400352620"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd /content/drive/My\\ Drive/Code/Project/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/Code/Project\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p0vsAiRW_heG","colab_type":"text"},"source":["## Install Packages"]},{"cell_type":"code","metadata":{"id":"-jXduRQKoMhP","colab_type":"code","colab":{}},"source":["%%capture\n","installed_packages = !pip list\n","# required_packages = ['pycuda', 'scipy', 'hiddenlayer', 'sklearn']\n","required_packages = ['pycuda']\n","for x in required_packages:\n","  if x not in installed_packages:\n","    print(x)\n","    null_capture = !pip install $x\n","\n","print('Packages checked/installed')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvB9byoV_nLG","colab_type":"text"},"source":["# Model, Dataset and Hyperparameters"]},{"cell_type":"code","metadata":{"id":"DHO8c_7e_sd3","colab_type":"code","colab":{}},"source":["# MobileNetV2\t|\tresnet18 (34,50,101,152)\t|\tConvNet\t|\tGoogleNet\n","model_name = 'resnet18'\n","data_name = 'CIFAR10'\n","\n","model_name = model_name.lower()\n","WEIGHTS = 32\n","EPOCH_NUM = 30\n","\n","NUM_WORKERS = 8\n","BATCH_SIZE = 16\n","\n","HP_momentum = 0.9\n","HP_weightdecay = 0.00004\n","\n","LR = 0.1 # NOTE: LR is adaptive\n","# Adaptive LR specific parameters\n","LR_PATIENCE = 3 # This is the number of epochs to observe no change in before change LR\n","LR_FACTOR = 0.9  # factor by which to reduce the learning rate: newLR = oldLR*LF_FACTOR"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZ-sQ9VR_YG9","colab_type":"text"},"source":["# Load Data and Model"]},{"cell_type":"code","metadata":{"id":"k5Mj31cSvIhz","colab_type":"code","outputId":"c2bec6d5-8675-4bb9-dc46-2cfa202d0812","executionInfo":{"status":"ok","timestamp":1572486409640,"user_tz":420,"elapsed":8592,"user":{"displayName":"Aditya Kunapuli","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDdP8koEInjmAwkN_phGm2BjZqMmpOhU2nNPAiIWg=s64","userId":"12383070090400352620"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import model_Load\n","import importlib\n","importlib.reload(model_Load) # forces a refresh in case of any changes to imported py package\n","\n","LoadModel = model_Load.LoadModel()\n","# Get model-specific transforms\n","trainT, testT = LoadModel.transform_type(model_name)\n","# Load chosen dataset\n","train_loader, test_loader, classes = LoadModel.load_data(dataset_name=data_name,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\ttransform_train=trainT, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\ttransform_test=testT, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tbatchsize=BATCH_SIZE, \n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tnumworkers=NUM_WORKERS)\n","\n","import importlib\n","# Load chosen model\n","if ('mobile' in model_name and 'v2' not in model_name):\n","\tfrom model_MobileNet import MobileNet as model\n","\tprint('MobileNet')\n","elif ('mobile' in model_name and 'v2' in model_name):\n","\tfrom torchvision.models import mobilenet_v2 as model\n","\tprint('MobileNetV2')\n","elif ('resnet' in model_name):\n","\tmodel = getattr(importlib.import_module('torchvision.models'), model_name)\n","\tprint('ResNet') \n","elif 'conv' in model_name:\n","\tfrom model_ConvNet import ConvNet as model\n","\tprint('ConvNet')\n","elif 'google' in model_name:\n","\tfrom model_GoogleNet import GoogleNet as model\n","\tprint('GoogleNet')\n","else: \n","\tprint('Choose a correct model type: MobileNet, MobileNetV2, GoogleNet, ConvNet')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["general\n","Dataset: CIFAR10\n","Classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n","ResNet\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8sALciwOXggi","colab_type":"text"},"source":["# Define Loss Function"]},{"cell_type":"code","metadata":{"id":"yCh47sh8Xh2t","colab_type":"code","colab":{}},"source":["# criterion = nn.CrossEntropyLoss()\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FocalLoss(nn.Module):\n","\tdef __init__(self, alpha = 1, gamma = 2):\n","\t\tsuper(FocalLoss, self).__init__()\n","\t\tself.alpha = alpha\n","\t\tself.gamma = gamma\n","\t\n","\tdef forward(self, inputs, target):\n","\t\tBCE_loss = -F.cross_entropy(inputs, target, reduction='none') # disable reduction=mean\n","\t\tpt = torch.exp(BCE_loss)\n","\t\tF_loss = -self.alpha * (1 - pt) ** self.gamma * BCE_loss\n","\t\treturn torch.mean(F_loss)\n","\n","criterion = FocalLoss(alpha=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_sucWROy5ksm","colab_type":"text"},"source":["# Define the *```main```* function"]},{"cell_type":"code","metadata":{"id":"dK0ZzxmL5pfe","colab_type":"code","colab":{}},"source":["import torch\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lrs\n","from torchsummary import summary\n","from timeit import default_timer as timer\n","from datetime import timedelta\n","import math\n","import os\n","import numpy as np\n","from scipy import io as sio\n","import pycuda.driver as cuda\n","cuda.init()\n","torch.cuda.current_device() \n","cuda.Device(0).name()\n","\n","def main():\n","\t# ########################################################################\n","\t# # Define Loss function and optimizer\n","\t# ########################################################################\n","\t# criterion = nn.CrossEntropyLoss()\n","\toptimizer = optim.SGD(net.parameters(), lr = LR, momentum = HP_momentum, weight_decay = HP_weightdecay)\n","\t # Adaptive learning rate--if accuracy (over 2 epochs) doesn't increase then reduce it by 10%\n","\tscheduler = lrs.ReduceLROnPlateau(optimizer, 'max',\n","\t\t\t\t\t\t\t\t\t   factor = LR_FACTOR,\n","\t\t\t\t\t\t\t\t\t   patience = LR_PATIENCE,\n","\t\t\t\t\t\t\t\t\t   verbose = True,\n","\t\t\t\t\t\t\t\t\t   threshold = 1)\n","\n","\t# scheduler = CyclicLR(optimizer, base_lr = 0.0001, max_lr = 0.1, step_size = half_cycle)  \n","\t########################################################################\n","\t# Begin training\n","\t########################################################################\n","\t# Print model hyper-parameters and hidden parameters\n","\tprint('=' * 100)\n","\tprint('Classes: [%s]' % ', '.join(map(str, classes)))\n","\tprint('Number of Filters: ' + str(WEIGHTS))\n","\tprint('Batchsize = ' + str(BATCH_SIZE))\n","\tprint('Number of Batches = ' + str(train_iterations))\n","\tsummary(net, (3, 224, 224))\n","\tprint('=' * 100)\n","\ttest_accuracy = []\n","\ttrain_accuracy = []\n","\ttrain_loss = []\n","\t# history2 = hl.History()\n","\t# canvas2 = hl.Canvas() \n","\tstep = (0,0)\n","\t# net.load_state_dict(torch.load(checkpoint_path))\n","\tfor epoch in range(1, EPOCH_NUM, 1):  \n","\t\t# scheduler.batch_step()  # uncomment for CyclicLR\n","\t\tprint('Beginning Epoch ' + str(epoch) + ' with Learning Rate = ' + str(round(get_lr(optimizer), 5)))\n","\t\trunning_loss, test_min_acc, total, correct = (0.0, 0.0, 0, 0)\n","\t\tfor i, Data in enumerate(train_loader, 0):\n","\t\t\tstep = (epoch, i)\n","\t\t\tnet.train()\n","\t\t\t# get the inputs\n","\t\t\tinputs, labels = Data\n","\t\t\t# zero the parameter gradients\n","\t\t\toptimizer.zero_grad()\n","\t\t\t# forward + backward + optimize\n","\t\t\toutputs = net(inputs.to(device)).to(device)\n","\t\t\tloss = criterion(outputs, labels.to(device))\n","\t\t\tloss.backward()\n","\t\t\toptimizer.step()\n","\t\t\t# print statistics\n","\t\t\trunning_loss += loss.item()\n","\t\t\tnet.eval()\n","\t\t\t_, predicted = torch.max(outputs.data, 1)\n","\t\t\t# Accuracy of given batch\n","\t\t\ttotal += labels.size(0)\n","\t\t\tcorrect += (predicted == labels.to(device)).sum().item()\n","\t\t\ttrain_loss.append(running_loss / 20)\n","\t\t\ttrain_accuracy.append(100.0 * correct / total)\n","\t\t\taccuracy = 100.0 * correct / total\n","\n","\t\t\tif i % 200 == 0:  # print every 20 mini-batches\n","\t\t\t\tprint('Train: [%d, %5d] Loss: %.3f Acc: %.3f' % (epoch, i + 1, running_loss / 20, accuracy))\n","\t\t\t\trunning_loss = 0.0\n","\t\t\t\n","\t\t# TEST LEARNT MODEL ON TEST-SET\n","\t\tcorrect, total = (0, 0)\n","\t\twith torch.no_grad():\n","\t\t\tnet.eval()\n","\t\t\tfor Data in test_loader:\n","\t\t\t\timages, labels = Data\n","\t\t\t\toutputs = net(images.to(device))\n","\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n","\t\t\t\ttotal += labels.to(device).size(0)\n","\t\t\t\tcorrect += (predicted == labels.to(device)).sum().item()\n","\t\ttest_accuracy.append(100.0 * correct / total)\n","\t\ttest_ep_acc = test_accuracy[-1]\n","\t\t# See if LR needs to be changed--used by ReduceLROnPlateau above\n","\t\tscheduler.step(test_ep_acc)\n","\t\t\n","\t\ttest_acc_str = '[ Epoch ' + str(epoch) + ' Test Accuracy = ' + str(round(test_ep_acc, 3)) + ' % ]'\n","\t\tpad = 50 - int(round(float(len(test_acc_str) / 2)))\n","\t\tprint('=' * pad + test_acc_str + '=' * pad)\n","\t\t# SAVE BEST MODEL\n","\t\tif test_min_acc < test_ep_acc:\n","\t\t\ttest_min_acc = test_ep_acc\n","\t\t\ttorch.save(net, MODEL_SAVE_PATH + '/my_best_model.pth')\n","\t\n","\tnp.save('test_accuracy.npy', test_accuracy);\n","\tsio.savemat('test_accuracy.mat', mdict = {'test_accuracy': test_accuracy})\n","\tnp.save('train_accuracy.npy', train_accuracy);\n","\tsio.savemat('train_accuracy.mat', mdict = {'train_accuracy': train_accuracy})\n","\tnp.save('train_loss.npy', train_loss);\n","\tsio.savemat('train_loss.mat', mdict = {'train_loss': train_loss})\n","\t\n","\tprint('Finished Training')  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_Uy6wco6L5n","colab_type":"text"},"source":["# Execute"]},{"cell_type":"code","metadata":{"id":"U30Px85r6MIl","colab_type":"code","outputId":"17b8cdc5-d413-4249-e515-2d12d9124821","executionInfo":{"status":"ok","timestamp":1572350747950,"user_tz":420,"elapsed":6849604,"user":{"displayName":"Aditya Kunapuli","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDdP8koEInjmAwkN_phGm2BjZqMmpOhU2nNPAiIWg=s64","userId":"12383070090400352620"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train_iterations = int((50000/BATCH_SIZE)) \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","def get_lr(optimizer):\n","\tfor param_group in optimizer.param_groups:\n","\t\treturn param_group['lr']    \n","\n","######################################################\n","################ CREATE MODEL SUB-DIR ################\n","######################################################\n","img_dir = './data' # Specificy path to CIFAR-10 dataset and set download yes/no flag\n","MODEL_SAVE_PATH = './Output/'+ model_name\n","cwd_path = os.getcwd()\n","if cwd_path == '/content': # Check current directory isn't root\n","    %cd /content/drive/My\\ Drive/Code/Project\n","mkdir_var = str(cwd_path +  '/Output/' + model_name + '/Models').replace(\" \", \"\\ \")\n","print(mkdir_var)\n","!mkdir -p $mkdir_var\n","print(MODEL_SAVE_PATH)\n","# checkpoint_path = MODEL_SAVE_PATH + '/' + model_name + '.pth'\n","checkpoint_path = MODEL_SAVE_PATH + '/my_best_model.pth'\n","\n","\n","\n","######################################################\n","##################### EXECUTE ########################\n","######################################################\n","if __name__ == \"__main__\":\n","    # net = model().to(device)\n","    # net.load_state_dict(torch.load(checkpoint_path))\n","    if os.path.exists(checkpoint_path):\n","        net = torch.load(checkpoint_path)\n","    else:\n","        net = model().to(device)\n","    start = timer()\n","    main()\n","    end = timer()\n","    execution_time = timedelta(seconds=end-start)\n","    print(execution_time)\n","    # torch.save(net.state_dict(), checkpoint_path)\n","    file = open(MODEL_SAVE_PATH + '/ExecutionTime.txt','w')\n","    file.writelines(str(execution_time)) \n","    file.close() \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My\\ Drive/Code/Project/Output/resnet18/Models\n","./Output/resnet18\n","====================================================================================================\n","Classes: [plane, car, bird, cat, deer, dog, frog, horse, ship, truck]\n","Number of Filters: 32\n","Batchsize = 16\n","Number of Batches = 3125\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-20          [-1, 128, 28, 28]             256\n","             ReLU-21          [-1, 128, 28, 28]               0\n","           Conv2d-22          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-23          [-1, 128, 28, 28]             256\n","           Conv2d-24          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-25          [-1, 128, 28, 28]             256\n","             ReLU-26          [-1, 128, 28, 28]               0\n","       BasicBlock-27          [-1, 128, 28, 28]               0\n","           Conv2d-28          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-29          [-1, 128, 28, 28]             256\n","             ReLU-30          [-1, 128, 28, 28]               0\n","           Conv2d-31          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-36          [-1, 256, 14, 14]             512\n","             ReLU-37          [-1, 256, 14, 14]               0\n","           Conv2d-38          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-39          [-1, 256, 14, 14]             512\n","           Conv2d-40          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-41          [-1, 256, 14, 14]             512\n","             ReLU-42          [-1, 256, 14, 14]               0\n","       BasicBlock-43          [-1, 256, 14, 14]               0\n","           Conv2d-44          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-45          [-1, 256, 14, 14]             512\n","             ReLU-46          [-1, 256, 14, 14]               0\n","           Conv2d-47          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-48          [-1, 256, 14, 14]             512\n","             ReLU-49          [-1, 256, 14, 14]               0\n","       BasicBlock-50          [-1, 256, 14, 14]               0\n","           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n","             ReLU-53            [-1, 512, 7, 7]               0\n","           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n","           Conv2d-56            [-1, 512, 7, 7]         131,072\n","      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n","             ReLU-58            [-1, 512, 7, 7]               0\n","       BasicBlock-59            [-1, 512, 7, 7]               0\n","           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n","             ReLU-62            [-1, 512, 7, 7]               0\n","           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n","             ReLU-65            [-1, 512, 7, 7]               0\n","       BasicBlock-66            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","           Linear-68                 [-1, 1000]         513,000\n","================================================================\n","Total params: 11,689,512\n","Trainable params: 11,689,512\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 62.79\n","Params size (MB): 44.59\n","Estimated Total Size (MB): 107.96\n","----------------------------------------------------------------\n","====================================================================================================\n","Beginning Epoch 1 with Learning Rate = 0.1\n","Train: [1,     1] Loss: 0.365 Acc: 0.000\n","Train: [1,   201] Loss: 27.952 Acc: 12.500\n","Train: [1,   401] Loss: 17.461 Acc: 15.290\n","Train: [1,   601] Loss: 17.037 Acc: 16.764\n","Train: [1,   801] Loss: 16.665 Acc: 17.580\n","Train: [1,  1001] Loss: 15.951 Acc: 18.531\n","Train: [1,  1201] Loss: 15.355 Acc: 19.603\n","Train: [1,  1401] Loss: 15.325 Acc: 20.294\n","Train: [1,  1601] Loss: 14.539 Acc: 21.217\n","Train: [1,  1801] Loss: 14.471 Acc: 21.849\n","Train: [1,  2001] Loss: 14.309 Acc: 22.679\n","Train: [1,  2201] Loss: 14.271 Acc: 23.254\n","Train: [1,  2401] Loss: 13.793 Acc: 23.904\n","Train: [1,  2601] Loss: 13.739 Acc: 24.435\n","Train: [1,  2801] Loss: 13.459 Acc: 24.998\n","Train: [1,  3001] Loss: 13.387 Acc: 25.464\n","================================[ Epoch 1 Test Accuracy = 41.36 % ]================================\n","Beginning Epoch 2 with Learning Rate = 0.1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train: [2,     1] Loss: 0.069 Acc: 25.000\n","Train: [2,   201] Loss: 13.072 Acc: 32.960\n","Train: [2,   401] Loss: 12.969 Acc: 33.370\n","Train: [2,   601] Loss: 12.946 Acc: 33.579\n","Train: [2,   801] Loss: 12.693 Acc: 33.513\n","Train: [2,  1001] Loss: 12.694 Acc: 33.741\n","Train: [2,  1201] Loss: 12.587 Acc: 34.055\n","Train: [2,  1401] Loss: 12.417 Acc: 34.279\n","Train: [2,  1601] Loss: 12.192 Acc: 34.447\n","Train: [2,  1801] Loss: 11.973 Acc: 34.873\n","Train: [2,  2001] Loss: 11.712 Acc: 35.401\n","Train: [2,  2201] Loss: 11.612 Acc: 35.751\n","Train: [2,  2401] Loss: 11.505 Acc: 36.172\n","Train: [2,  2601] Loss: 11.110 Acc: 36.510\n","Train: [2,  2801] Loss: 11.283 Acc: 36.833\n","Train: [2,  3001] Loss: 11.682 Acc: 36.990\n","=================================[ Epoch 2 Test Accuracy = 49.8 % ]=================================\n","Beginning Epoch 3 with Learning Rate = 0.1\n","Train: [3,     1] Loss: 0.071 Acc: 43.750\n","Train: [3,   201] Loss: 11.116 Acc: 41.573\n","Train: [3,   401] Loss: 10.827 Acc: 41.989\n","Train: [3,   601] Loss: 11.017 Acc: 42.169\n","Train: [3,   801] Loss: 10.945 Acc: 42.353\n","Train: [3,  1001] Loss: 10.347 Acc: 42.795\n","Train: [3,  1201] Loss: 10.459 Acc: 43.115\n","Train: [3,  1401] Loss: 10.243 Acc: 43.389\n","Train: [3,  1601] Loss: 9.995 Acc: 43.840\n","Train: [3,  1801] Loss: 10.035 Acc: 44.062\n","Train: [3,  2001] Loss: 10.185 Acc: 44.312\n","Train: [3,  2201] Loss: 10.359 Acc: 44.409\n","Train: [3,  2401] Loss: 9.971 Acc: 44.614\n","Train: [3,  2601] Loss: 9.380 Acc: 45.060\n","Train: [3,  2801] Loss: 9.699 Acc: 45.352\n","Train: [3,  3001] Loss: 9.855 Acc: 45.447\n","=================================[ Epoch 3 Test Accuracy = 57.4 % ]=================================\n","Beginning Epoch 4 with Learning Rate = 0.1\n","Train: [4,     1] Loss: 0.034 Acc: 68.750\n","Train: [4,   201] Loss: 9.274 Acc: 49.627\n","Train: [4,   401] Loss: 9.424 Acc: 49.112\n","Train: [4,   601] Loss: 9.710 Acc: 48.908\n","Train: [4,   801] Loss: 9.289 Acc: 49.103\n","Train: [4,  1001] Loss: 9.363 Acc: 49.457\n","Train: [4,  1201] Loss: 9.271 Acc: 49.636\n","Train: [4,  1401] Loss: 9.055 Acc: 49.830\n","Train: [4,  1601] Loss: 8.740 Acc: 50.215\n","Train: [4,  1801] Loss: 8.793 Acc: 50.382\n","Train: [4,  2001] Loss: 8.835 Acc: 50.503\n","Train: [4,  2201] Loss: 8.560 Acc: 50.755\n","Train: [4,  2401] Loss: 8.603 Acc: 51.044\n","Train: [4,  2601] Loss: 8.709 Acc: 51.168\n","Train: [4,  2801] Loss: 8.682 Acc: 51.328\n","Train: [4,  3001] Loss: 8.523 Acc: 51.454\n","================================[ Epoch 4 Test Accuracy = 65.39 % ]================================\n","Beginning Epoch 5 with Learning Rate = 0.1\n","Train: [5,     1] Loss: 0.034 Acc: 62.500\n","Train: [5,   201] Loss: 8.484 Acc: 53.109\n","Train: [5,   401] Loss: 8.403 Acc: 53.850\n","Train: [5,   601] Loss: 8.542 Acc: 53.650\n","Train: [5,   801] Loss: 8.488 Acc: 53.784\n","Train: [5,  1001] Loss: 8.337 Acc: 54.121\n","Train: [5,  1201] Loss: 8.078 Acc: 54.314\n","Train: [5,  1401] Loss: 8.044 Acc: 54.577\n","Train: [5,  1601] Loss: 7.901 Acc: 54.704\n","Train: [5,  1801] Loss: 8.188 Acc: 54.779\n","Train: [5,  2001] Loss: 8.298 Acc: 54.848\n","Train: [5,  2201] Loss: 7.923 Acc: 55.023\n","Train: [5,  2401] Loss: 8.016 Acc: 55.131\n","Train: [5,  2601] Loss: 7.896 Acc: 55.322\n","Train: [5,  2801] Loss: 7.816 Acc: 55.462\n","Train: [5,  3001] Loss: 7.772 Acc: 55.544\n","Epoch     4: reducing learning rate of group 0 to 9.0000e-02.\n","================================[ Epoch 5 Test Accuracy = 69.42 % ]================================\n","Beginning Epoch 6 with Learning Rate = 0.09\n","Train: [6,     1] Loss: 0.072 Acc: 37.500\n","Train: [6,   201] Loss: 7.533 Acc: 58.986\n","Train: [6,   401] Loss: 7.369 Acc: 59.398\n","Train: [6,   601] Loss: 7.419 Acc: 59.723\n","Train: [6,   801] Loss: 7.109 Acc: 60.190\n","Train: [6,  1001] Loss: 7.438 Acc: 60.059\n","Train: [6,  1201] Loss: 7.564 Acc: 59.784\n","Train: [6,  1401] Loss: 7.403 Acc: 59.578\n","Train: [6,  1601] Loss: 7.343 Acc: 59.627\n","Train: [6,  1801] Loss: 7.329 Acc: 59.540\n","Train: [6,  2001] Loss: 6.820 Acc: 59.758\n","Train: [6,  2201] Loss: 7.007 Acc: 59.774\n","Train: [6,  2401] Loss: 7.268 Acc: 59.720\n","Train: [6,  2601] Loss: 6.862 Acc: 59.907\n","Train: [6,  2801] Loss: 7.394 Acc: 59.865\n","Train: [6,  3001] Loss: 7.283 Acc: 59.874\n","================================[ Epoch 6 Test Accuracy = 71.39 % ]================================\n","Beginning Epoch 7 with Learning Rate = 0.09\n","Train: [7,     1] Loss: 0.022 Acc: 68.750\n","Train: [7,   201] Loss: 6.830 Acc: 62.500\n","Train: [7,   401] Loss: 6.831 Acc: 61.861\n","Train: [7,   601] Loss: 6.977 Acc: 61.273\n","Train: [7,   801] Loss: 6.820 Acc: 61.306\n","Train: [7,  1001] Loss: 6.957 Acc: 61.220\n","Train: [7,  1201] Loss: 7.043 Acc: 61.371\n","Train: [7,  1401] Loss: 6.848 Acc: 61.483\n","Train: [7,  1601] Loss: 6.786 Acc: 61.622\n","Train: [7,  1801] Loss: 6.869 Acc: 61.757\n","Train: [7,  2001] Loss: 6.688 Acc: 61.925\n","Train: [7,  2201] Loss: 7.053 Acc: 61.784\n","Train: [7,  2401] Loss: 6.869 Acc: 61.784\n","Train: [7,  2601] Loss: 6.735 Acc: 61.765\n","Train: [7,  2801] Loss: 6.596 Acc: 61.835\n","Train: [7,  3001] Loss: 6.775 Acc: 61.942\n","================================[ Epoch 7 Test Accuracy = 74.35 % ]================================\n","Beginning Epoch 8 with Learning Rate = 0.09\n","Train: [8,     1] Loss: 0.047 Acc: 37.500\n","Train: [8,   201] Loss: 6.260 Acc: 63.899\n","Train: [8,   401] Loss: 6.524 Acc: 63.778\n","Train: [8,   601] Loss: 6.314 Acc: 64.008\n","Train: [8,   801] Loss: 6.836 Acc: 63.561\n","Train: [8,  1001] Loss: 6.808 Acc: 63.412\n","Train: [8,  1201] Loss: 6.549 Acc: 63.458\n","Train: [8,  1401] Loss: 6.366 Acc: 63.539\n","Train: [8,  1601] Loss: 6.500 Acc: 63.659\n","Train: [8,  1801] Loss: 6.809 Acc: 63.520\n","Train: [8,  2001] Loss: 6.518 Acc: 63.515\n","Train: [8,  2201] Loss: 6.525 Acc: 63.511\n","Train: [8,  2401] Loss: 6.388 Acc: 63.648\n","Train: [8,  2601] Loss: 6.466 Acc: 63.745\n","Train: [8,  2801] Loss: 6.583 Acc: 63.738\n","Train: [8,  3001] Loss: 6.334 Acc: 63.714\n","================================[ Epoch 8 Test Accuracy = 76.06 % ]================================\n","Beginning Epoch 9 with Learning Rate = 0.09\n","Train: [9,     1] Loss: 0.034 Acc: 62.500\n","Train: [9,   201] Loss: 6.179 Acc: 65.609\n","Train: [9,   401] Loss: 6.027 Acc: 65.789\n","Train: [9,   601] Loss: 6.257 Acc: 65.713\n","Train: [9,   801] Loss: 6.083 Acc: 65.777\n","Train: [9,  1001] Loss: 6.062 Acc: 65.659\n","Train: [9,  1201] Loss: 6.055 Acc: 65.752\n","Train: [9,  1401] Loss: 6.176 Acc: 65.725\n","Train: [9,  1601] Loss: 5.958 Acc: 65.869\n","Train: [9,  1801] Loss: 6.034 Acc: 65.960\n","Train: [9,  2001] Loss: 6.292 Acc: 65.783\n","Train: [9,  2201] Loss: 6.644 Acc: 65.530\n","Train: [9,  2401] Loss: 6.156 Acc: 65.494\n","Train: [9,  2601] Loss: 6.114 Acc: 65.569\n","Train: [9,  2801] Loss: 6.124 Acc: 65.586\n","Train: [9,  3001] Loss: 6.021 Acc: 65.636\n","Epoch     8: reducing learning rate of group 0 to 8.1000e-02.\n","================================[ Epoch 9 Test Accuracy = 75.84 % ]================================\n","Beginning Epoch 10 with Learning Rate = 0.081\n","Train: [10,     1] Loss: 0.037 Acc: 56.250\n"],"name":"stdout"}]}]}